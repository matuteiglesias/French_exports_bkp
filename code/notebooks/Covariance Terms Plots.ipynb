{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#1f77b4',\n",
       " '#ff7f0e',\n",
       " '#2ca02c',\n",
       " '#d62728',\n",
       " '#9467bd',\n",
       " '#8c564b',\n",
       " '#e377c2',\n",
       " '#7f7f7f',\n",
       " '#bcbd22',\n",
       " '#17becf']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "pal = sns.color_palette('tab10')\n",
    "pal.as_hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df = pd.read_csv('./../../data/processed/.csv')\n",
    "\n",
    "# sales = df.loc[df.IMPORT == 1].groupby(['ID', 'YEAR'])['VART'].sum().unstack()\n",
    "\n",
    "# sales = sales.loc[sales.sum(1).sort_values().index]\n",
    "\n",
    "# logsales = np.log10(sales)\n",
    "# demlogsales = logsales.subtract(logsales.mean(1), axis = 0)\n",
    "\n",
    "# sizes = sales.sum(1)\n",
    "\n",
    "\n",
    "# Q = 10\n",
    "# parts = pd.cut(sizes.cumsum()/sizes.sum(), Q, labels = range(Q)).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sk = sales.groupby(parts).sum()\n",
    "# # yqs.T.cov().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = Sk.sum() # == sales.sum()\n",
    "# EX = X.mean()\n",
    "\n",
    "# # Exact\n",
    "# print(Sk.T.cov().sum().sum())\n",
    "# print(X.var())\n",
    "\n",
    "# # Aggregate approx\n",
    "# print(np.log10(X).var()*(np.log(10)*EX)**2)\n",
    "\n",
    "# # Sectoral log fluctuations\n",
    "# # Sq = yqs.mean(1).mean() # approx un sq que es el valor lineal medio de los qs\n",
    "# # Sq**2*np.log(10)**2*(np.log10(yqs).T.cov().sum().sum())\n",
    "# print((EX/Q)**2*np.log(10)**2*(np.log10(Sk).T.cov().sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logsales = np.log10(sales)\n",
    "\n",
    "# micro_s = logsales.sub(logsales.mean(1), axis=0).unstack().std()\n",
    "# zero_shock = logsales.notna().multiply(logsales.mean(1), axis = 0).replace(0, np.nan)\n",
    "\n",
    "# # noise = np.power(10, zero_shock + simu_shocks) - np.power(10, zero_shock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_R + noise_qs + base_qs - Sk == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# cm = sns.light_palette(\"green\", as_cmap=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nqs = parts.value_counts().values\n",
    "# lognqs = np.log10(nqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The outcome of bootstrap is having the zero shock time series of quantiles and the 'all included time series of quantiles'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additive decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### LOAD DATA\n",
    "\n",
    "# n = 100\n",
    "# Q = 10\n",
    "\n",
    "# # legacy\n",
    "# # size_sorting = False\n",
    "# # bs_base = pd.read_csv('./../../data/bootstraps/bs_base_'+['random', 'sorted'][size_sorting]+'_m'+str(n)+'_q'+str(Q)+'csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "# # bs_totl = pd.read_csv('./../../data/bootstraps/bs_totl_'+['random', 'sorted'][size_sorting]+'_m'+str(n)+'_q'+str(Q)+'csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "\n",
    "# bs_base = pd.read_csv('./../../data/bootstraps/bs_base_m'+str(n)+'_q'+str(Q)+'_'+['X', 'M'][i]+'.csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "# bs_totl = pd.read_csv('./../../data/bootstraps/bs_totl_m'+str(n)+'_q'+str(Q)+'_'+['X', 'M'][i]+'.csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "\n",
    "# bs_base = bs_base.loc[bs_base.parts == 'P'].drop('parts', axis = 1)\n",
    "# bs_totl = bs_totl.loc[bs_totl.parts == 'P'].drop('parts', axis = 1)\n",
    "\n",
    "# ### ADDITIVE DECOMPOSITION\n",
    "\n",
    "# bs_base = bs_base.groupby(level = ['s', 'q', 'm']).sum()  # drops the nq, it's not really grouping\n",
    "# bs_totl = bs_totl.groupby(level = ['s', 'q', 'm']).sum()  # drops the nq, it's not really grouping\n",
    "# fe_n = (bs_totl - bs_base)\n",
    "# fe = fe_n.groupby(level = ['s', 'q']).transform('median') \n",
    "# noise = fe_n - fe\n",
    "\n",
    "# ### MULTIPLICATIVE DECOMPOSITION\n",
    "\n",
    "# mul_fe_noise = bs_totl/bs_base\n",
    "# # Now, I use the bs to estimate disentangling fe from noise\n",
    "# mul_fe = mul_fe_noise.groupby(level = ['s', 'q']).transform('median') \n",
    "# mul_noise = mul_fe_noise/mul_fe\n",
    "\n",
    "# # Important: they fulfill\n",
    "# # bs_totl - bs_base*mul_fe*mul_noise == 0\n",
    "# # np.log10(bs_totl) - np.log10(bs_base) - np.log10(mul_fe) - np.log10(mul_noise) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Cov matrices of some experiments\n",
    "R = 5\n",
    "scale = 'lin'\n",
    "sorted_ = False\n",
    "n = 100\n",
    "Q = 10\n",
    "\n",
    "for i in [0, 1]:\n",
    "\n",
    "    ###########################################################\n",
    "\n",
    "\n",
    "    bs_base = pd.read_csv('./../../data/bootstraps/bs_base_m'+str(n)+'_q'+str(Q)+'_'+['X', 'M'][i]+'.csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "    bs_totl = pd.read_csv('./../../data/bootstraps/bs_totl_m'+str(n)+'_q'+str(Q)+'_'+['X', 'M'][i]+'.csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "\n",
    "    bs_base = bs_base.loc[bs_base.parts == 'P'].drop('parts', axis = 1)\n",
    "    bs_totl = bs_totl.loc[bs_totl.parts == 'P'].drop('parts', axis = 1)\n",
    "        \n",
    "    for r in range(R):\n",
    "    #     pick_m = np.random.choice(range(n))\n",
    "        pick_m = [3, 6, 2, 5, 8][r]\n",
    "\n",
    "        totl_ = bs_totl.loc[(bs_totl.index.get_level_values('m') == pick_m) & (bs_totl.index.get_level_values('s') == 0.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sectoral structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>q</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beta</th>\n",
       "      <td>3.158815e+09</td>\n",
       "      <td>4.683012e+09</td>\n",
       "      <td>4.803696e+09</td>\n",
       "      <td>2.294720e+09</td>\n",
       "      <td>4.889384e+09</td>\n",
       "      <td>2.907366e+09</td>\n",
       "      <td>7.444055e+09</td>\n",
       "      <td>3.966511e+09</td>\n",
       "      <td>4.083861e+09</td>\n",
       "      <td>2.633967e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "q                0             1             2             3             4  \\\n",
       "beta  3.158815e+09  4.683012e+09  4.803696e+09  2.294720e+09  4.889384e+09   \n",
       "\n",
       "q                5             6             7             8             9  \n",
       "beta  2.907366e+09  7.444055e+09  3.966511e+09  4.083861e+09  2.633967e+09  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fitting with the mean curve\n",
    "X = pd.DataFrame(totl_.mean())\n",
    "Y = totl_.values\n",
    "\n",
    "# This gives near one. \n",
    "betas = pd.DataFrame(np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(Y.T), ['beta'], totl_.index.get_level_values('q'))\n",
    "betas # It's not just the shares # totl_.sum(1)/totl_.sum().sum()\n",
    "\n",
    "\n",
    "## Fitting with a standardized line\n",
    "T = 17; t = np.arange(0, T)\n",
    "t_ = (t - t.mean())/t.std()\n",
    "X[0] = t_ # overwrite\n",
    "Y = (totl_.T - totl_.mean(1)).T.values # remove intercepts. # doesnt seem to change anything...\n",
    "\n",
    "# This gives near one. \n",
    "betas = pd.DataFrame(np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(Y.T), ['beta'], totl_.index.get_level_values('q'))\n",
    "betas # It's not just the shares # totl_.sum(1)/totl_.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05882353]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X.T.dot(X))#.dot(X.T).dot(Y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.15881477e+09, 4.68301239e+09, 4.80369551e+09, 2.29472012e+09,\n",
       "        4.88938383e+09, 2.90736638e+09, 7.44405483e+09, 3.96651085e+09,\n",
       "        4.08386056e+09, 2.63396725e+09]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(Y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.158815e+09</td>\n",
       "      <td>4.683012e+09</td>\n",
       "      <td>4.803696e+09</td>\n",
       "      <td>2.294720e+09</td>\n",
       "      <td>4.889384e+09</td>\n",
       "      <td>2.907366e+09</td>\n",
       "      <td>7.444055e+09</td>\n",
       "      <td>3.966511e+09</td>\n",
       "      <td>4.083861e+09</td>\n",
       "      <td>2.633967e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0             1             2             3             4  \\\n",
       "0  3.158815e+09  4.683012e+09  4.803696e+09  2.294720e+09  4.889384e+09   \n",
       "\n",
       "              5             6             7             8             9  \n",
       "0  2.907366e+09  7.444055e+09  3.966511e+09  4.083861e+09  2.633967e+09  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X.T).dot(Y.T).div((X**2).sum(), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.632993</td>\n",
       "      <td>-1.428869</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>-1.020621</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>-0.612372</td>\n",
       "      <td>-0.408248</td>\n",
       "      <td>-0.204124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.612372</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>1.020621</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>1.428869</td>\n",
       "      <td>1.632993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1997      1998      1999      2000      2001      2002      2003  \\\n",
       "0 -1.632993 -1.428869 -1.224745 -1.020621 -0.816497 -0.612372 -0.408248   \n",
       "\n",
       "       2004  2005      2006      2007      2008      2009      2010      2011  \\\n",
       "0 -0.204124   0.0  0.204124  0.408248  0.612372  0.816497  1.020621  1.224745   \n",
       "\n",
       "       2012      2013  \n",
       "0  1.428869  1.632993  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = totl_\n",
    "bX = pd.DataFrame(np.outer(betas.values, X. values), totl_.index, totl_.columns)\n",
    "r = y - bX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>q</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>m</th>\n",
       "      <th>8</th>\n",
       "      <th>8</th>\n",
       "      <th>8</th>\n",
       "      <th>8</th>\n",
       "      <th>8</th>\n",
       "      <th>8</th>\n",
       "      <th>8</th>\n",
       "      <th>8</th>\n",
       "      <th>8</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>nqs</th>\n",
       "      <th>25979</th>\n",
       "      <th>25565</th>\n",
       "      <th>22606</th>\n",
       "      <th>22198</th>\n",
       "      <th>22141</th>\n",
       "      <th>20285</th>\n",
       "      <th>19906</th>\n",
       "      <th>19904</th>\n",
       "      <th>18662</th>\n",
       "      <th>13064</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q</th>\n",
       "      <th>m</th>\n",
       "      <th>s</th>\n",
       "      <th>nqs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>8</th>\n",
       "      <th>0.5</th>\n",
       "      <th>25979</th>\n",
       "      <td>1.060174e+19</td>\n",
       "      <td>1.571732e+19</td>\n",
       "      <td>1.612236e+19</td>\n",
       "      <td>7.701633e+18</td>\n",
       "      <td>1.640995e+19</td>\n",
       "      <td>9.757821e+18</td>\n",
       "      <td>2.498404e+19</td>\n",
       "      <td>1.331257e+19</td>\n",
       "      <td>1.370642e+19</td>\n",
       "      <td>8.840228e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>8</th>\n",
       "      <th>0.5</th>\n",
       "      <th>25565</th>\n",
       "      <td>1.571732e+19</td>\n",
       "      <td>2.330127e+19</td>\n",
       "      <td>2.390175e+19</td>\n",
       "      <td>1.141784e+19</td>\n",
       "      <td>2.432811e+19</td>\n",
       "      <td>1.446618e+19</td>\n",
       "      <td>3.703939e+19</td>\n",
       "      <td>1.973617e+19</td>\n",
       "      <td>2.032007e+19</td>\n",
       "      <td>1.310583e+19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>8</th>\n",
       "      <th>0.5</th>\n",
       "      <th>22606</th>\n",
       "      <td>1.612236e+19</td>\n",
       "      <td>2.390175e+19</td>\n",
       "      <td>2.451771e+19</td>\n",
       "      <td>1.171208e+19</td>\n",
       "      <td>2.495506e+19</td>\n",
       "      <td>1.483898e+19</td>\n",
       "      <td>3.799391e+19</td>\n",
       "      <td>2.024478e+19</td>\n",
       "      <td>2.084372e+19</td>\n",
       "      <td>1.344358e+19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>8</th>\n",
       "      <th>0.5</th>\n",
       "      <th>22198</th>\n",
       "      <td>7.701633e+18</td>\n",
       "      <td>1.141784e+19</td>\n",
       "      <td>1.171208e+19</td>\n",
       "      <td>5.594849e+18</td>\n",
       "      <td>1.192100e+19</td>\n",
       "      <td>7.088567e+18</td>\n",
       "      <td>1.814965e+19</td>\n",
       "      <td>9.670909e+18</td>\n",
       "      <td>9.957024e+18</td>\n",
       "      <td>6.421981e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>8</th>\n",
       "      <th>0.5</th>\n",
       "      <th>22141</th>\n",
       "      <td>1.640995e+19</td>\n",
       "      <td>2.432811e+19</td>\n",
       "      <td>2.495506e+19</td>\n",
       "      <td>1.192100e+19</td>\n",
       "      <td>2.540020e+19</td>\n",
       "      <td>1.510368e+19</td>\n",
       "      <td>3.867164e+19</td>\n",
       "      <td>2.060591e+19</td>\n",
       "      <td>2.121553e+19</td>\n",
       "      <td>1.368338e+19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>8</th>\n",
       "      <th>0.5</th>\n",
       "      <th>20285</th>\n",
       "      <td>9.757821e+18</td>\n",
       "      <td>1.446618e+19</td>\n",
       "      <td>1.483898e+19</td>\n",
       "      <td>7.088567e+18</td>\n",
       "      <td>1.510368e+19</td>\n",
       "      <td>8.981078e+18</td>\n",
       "      <td>2.299526e+19</td>\n",
       "      <td>1.225286e+19</td>\n",
       "      <td>1.261536e+19</td>\n",
       "      <td>8.136527e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>8</th>\n",
       "      <th>0.5</th>\n",
       "      <th>19906</th>\n",
       "      <td>2.498404e+19</td>\n",
       "      <td>3.703939e+19</td>\n",
       "      <td>3.799391e+19</td>\n",
       "      <td>1.814965e+19</td>\n",
       "      <td>3.867164e+19</td>\n",
       "      <td>2.299526e+19</td>\n",
       "      <td>5.887732e+19</td>\n",
       "      <td>3.137236e+19</td>\n",
       "      <td>3.230051e+19</td>\n",
       "      <td>2.083286e+19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>0.5</th>\n",
       "      <th>19904</th>\n",
       "      <td>1.331257e+19</td>\n",
       "      <td>1.973617e+19</td>\n",
       "      <td>2.024478e+19</td>\n",
       "      <td>9.670909e+18</td>\n",
       "      <td>2.060591e+19</td>\n",
       "      <td>1.225286e+19</td>\n",
       "      <td>3.137236e+19</td>\n",
       "      <td>1.671653e+19</td>\n",
       "      <td>1.721109e+19</td>\n",
       "      <td>1.110064e+19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>8</th>\n",
       "      <th>0.5</th>\n",
       "      <th>18662</th>\n",
       "      <td>1.370642e+19</td>\n",
       "      <td>2.032007e+19</td>\n",
       "      <td>2.084372e+19</td>\n",
       "      <td>9.957024e+18</td>\n",
       "      <td>2.121553e+19</td>\n",
       "      <td>1.261536e+19</td>\n",
       "      <td>3.230051e+19</td>\n",
       "      <td>1.721109e+19</td>\n",
       "      <td>1.772029e+19</td>\n",
       "      <td>1.142905e+19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <th>8</th>\n",
       "      <th>0.5</th>\n",
       "      <th>13064</th>\n",
       "      <td>8.840228e+18</td>\n",
       "      <td>1.310583e+19</td>\n",
       "      <td>1.344358e+19</td>\n",
       "      <td>6.421981e+18</td>\n",
       "      <td>1.368338e+19</td>\n",
       "      <td>8.136527e+18</td>\n",
       "      <td>2.083286e+19</td>\n",
       "      <td>1.110064e+19</td>\n",
       "      <td>1.142905e+19</td>\n",
       "      <td>7.371395e+18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "q                         0             1             2             3  \\\n",
       "m                         8             8             8             8   \n",
       "s                       0.5           0.5           0.5           0.5   \n",
       "nqs                   25979         25565         22606         22198   \n",
       "q m s   nqs                                                             \n",
       "0 8 0.5 25979  1.060174e+19  1.571732e+19  1.612236e+19  7.701633e+18   \n",
       "1 8 0.5 25565  1.571732e+19  2.330127e+19  2.390175e+19  1.141784e+19   \n",
       "2 8 0.5 22606  1.612236e+19  2.390175e+19  2.451771e+19  1.171208e+19   \n",
       "3 8 0.5 22198  7.701633e+18  1.141784e+19  1.171208e+19  5.594849e+18   \n",
       "4 8 0.5 22141  1.640995e+19  2.432811e+19  2.495506e+19  1.192100e+19   \n",
       "5 8 0.5 20285  9.757821e+18  1.446618e+19  1.483898e+19  7.088567e+18   \n",
       "6 8 0.5 19906  2.498404e+19  3.703939e+19  3.799391e+19  1.814965e+19   \n",
       "7 8 0.5 19904  1.331257e+19  1.973617e+19  2.024478e+19  9.670909e+18   \n",
       "8 8 0.5 18662  1.370642e+19  2.032007e+19  2.084372e+19  9.957024e+18   \n",
       "9 8 0.5 13064  8.840228e+18  1.310583e+19  1.344358e+19  6.421981e+18   \n",
       "\n",
       "q                         4             5             6             7  \\\n",
       "m                         8             8             8             8   \n",
       "s                       0.5           0.5           0.5           0.5   \n",
       "nqs                   22141         20285         19906         19904   \n",
       "q m s   nqs                                                             \n",
       "0 8 0.5 25979  1.640995e+19  9.757821e+18  2.498404e+19  1.331257e+19   \n",
       "1 8 0.5 25565  2.432811e+19  1.446618e+19  3.703939e+19  1.973617e+19   \n",
       "2 8 0.5 22606  2.495506e+19  1.483898e+19  3.799391e+19  2.024478e+19   \n",
       "3 8 0.5 22198  1.192100e+19  7.088567e+18  1.814965e+19  9.670909e+18   \n",
       "4 8 0.5 22141  2.540020e+19  1.510368e+19  3.867164e+19  2.060591e+19   \n",
       "5 8 0.5 20285  1.510368e+19  8.981078e+18  2.299526e+19  1.225286e+19   \n",
       "6 8 0.5 19906  3.867164e+19  2.299526e+19  5.887732e+19  3.137236e+19   \n",
       "7 8 0.5 19904  2.060591e+19  1.225286e+19  3.137236e+19  1.671653e+19   \n",
       "8 8 0.5 18662  2.121553e+19  1.261536e+19  3.230051e+19  1.721109e+19   \n",
       "9 8 0.5 13064  1.368338e+19  8.136527e+18  2.083286e+19  1.110064e+19   \n",
       "\n",
       "q                         8             9  \n",
       "m                         8             8  \n",
       "s                       0.5           0.5  \n",
       "nqs                   18662         13064  \n",
       "q m s   nqs                                \n",
       "0 8 0.5 25979  1.370642e+19  8.840228e+18  \n",
       "1 8 0.5 25565  2.032007e+19  1.310583e+19  \n",
       "2 8 0.5 22606  2.084372e+19  1.344358e+19  \n",
       "3 8 0.5 22198  9.957024e+18  6.421981e+18  \n",
       "4 8 0.5 22141  2.121553e+19  1.368338e+19  \n",
       "5 8 0.5 20285  1.261536e+19  8.136527e+18  \n",
       "6 8 0.5 19906  3.230051e+19  2.083286e+19  \n",
       "7 8 0.5 19904  1.721109e+19  1.110064e+19  \n",
       "8 8 0.5 18662  1.772029e+19  1.142905e+19  \n",
       "9 8 0.5 13064  1.142905e+19  7.371395e+18  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bX.T.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8168859562847974e+20"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.T.cov().sum().sum()  # it is about zero if fitting with the total line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.05604214624312e+21"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum().var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7743535506146397e+21"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bX.sum().var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8168859562847987e+20"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.sum().var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mat_data = pd.DataFrame(pd.concat([bX, r]).T.cov().values)\n",
    "sigsig = pd.DataFrame(np.outer(np.sqrt(np.diag(mat_data)), np.sqrt(np.diag(mat_data))))\n",
    "\n",
    "############################################\n",
    "# fig, axs = plt.subplots(1, R, figsize = (4*R, 4))\n",
    "fig, ax = plt.subplots(1, figsize = (4, 4))\n",
    "scale = 4e19\n",
    "# scale = 1.5\n",
    "\n",
    "im = ax.imshow(mat_data, vmin = -scale, vmax = scale, cmap = 'RdYlGn')\n",
    "\n",
    "# im = ax.imshow(mat_data > 1.5e18, vmin = -1, vmax = 1, cmap = 'RdYlGn')\n",
    "\n",
    "# Show all ticks...\n",
    "ax.set_xticks(np.arange(0, 2*Q, 10)); ax.set_yticks(np.arange(0, 2*Q, 10))\n",
    "# Show block lines\n",
    "ax.axhline(Q - .5, c = 'k'); ax.axvline(Q - .5, c = 'k')\n",
    "# plt.savefig('./../../../WRITING/paper1_writing/figures/OLS1_cov.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "############################################\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize = (4, 4))\n",
    "\n",
    "scale = 4e19\n",
    "im = ax.imshow(sigsig, vmin = -scale, vmax = scale, cmap = 'RdYlGn')\n",
    "\n",
    "# Show all ticks...\n",
    "ax.set_xticks(np.arange(0, 2*Q, 10)); ax.set_yticks(np.arange(0, 2*Q, 10))\n",
    "# Show block lines\n",
    "ax.axhline(Q - .5, c = 'k'); ax.axvline(Q - .5, c = 'k')\n",
    "# plt.savefig('./../../../WRITING/paper1_writing/figures/OLS1_cov_outersig.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "############################################\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize = (4, 4))\n",
    "\n",
    "scale = 1.5\n",
    "im = ax.imshow(mat_data/sigsig, vmin = -scale, vmax = scale, cmap = 'RdYlGn')\n",
    "# im = ax.imshow(mat_data > 1.5e18, vmin = -1, vmax = 1, cmap = 'RdYlGn')\n",
    "\n",
    "# Show all ticks...\n",
    "ax.set_xticks(np.arange(0, 2*Q, 10)); ax.set_yticks(np.arange(0, 2*Q, 10))\n",
    "# Show block lines\n",
    "ax.axhline(Q - .5, c = 'k'); ax.axvline(Q - .5, c = 'k')\n",
    "# plt.savefig('./../../../WRITING/paper1_writing/figures/OLS1_cov_div_outersig.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ############################################\n",
    "# # fig, axs = plt.subplots(1, R, figsize = (4*R, 4))\n",
    "# fig, ax = plt.subplots(1, figsize = (6, 6))\n",
    "# # scale = 4e19\n",
    "# scale = 1.5\n",
    "\n",
    "# im = ax.imshow((mat_data/sigsig) > .9999, vmin = -scale, vmax = scale, cmap = 'RdYlGn')\n",
    "\n",
    "# # im = ax.imshow(mat_data > 1.5e18, vmin = -1, vmax = 1, cmap = 'RdYlGn')\n",
    "\n",
    "# # Show all ticks...\n",
    "# ax.set_xticks(np.arange(0, 2*Q, 10)); ax.set_yticks(np.arange(0, 2*Q, 10))\n",
    "# # Show block lines\n",
    "# ax.axhline(Q - .5, c = 'k'); ax.axvline(Q - .5, c = 'k')\n",
    "# plt.savefig('./../../../WRITING/paper1_writing/figures/OLS_cov_3.png')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sums by blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    106.25\n",
       "dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*X.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0560421462431192e+21"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_data.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7743535506146391e+21"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_data.iloc[:10, :10].sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-64503.6125592267"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_data.iloc[10:, :10].sum().sum()### As if zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-64503.612559226705"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_data.iloc[:10, 10:].sum().sum() ### As if zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8168859562847974e+20"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_data.iloc[10:, 10:].sum().sum() ### As if zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/miglesia/anaconda2/envs/my_pymc_env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-84-78cfdecbfdf8>\", line 11, in <module>\n",
      "    bs_base = pd.read_csv('./../../data/bootstraps/bs_base_m'+str(n)+'_q'+str(Q)+'_'+['X', 'M'][i]+'.csv', index_col=['q', 'm', 's', 'nqs'])\n",
      "  File \"/home/miglesia/anaconda2/envs/my_pymc_env/lib/python3.6/site-packages/pandas/io/parsers.py\", line 702, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/home/miglesia/anaconda2/envs/my_pymc_env/lib/python3.6/site-packages/pandas/io/parsers.py\", line 411, in _read\n",
      "    compression = _infer_compression(filepath_or_buffer, compression)\n",
      "  File \"/home/miglesia/anaconda2/envs/my_pymc_env/lib/python3.6/site-packages/pandas/io/common.py\", line 293, in _infer_compression\n",
      "    filepath_or_buffer = _stringify_path(filepath_or_buffer)\n",
      "  File \"/home/miglesia/anaconda2/envs/my_pymc_env/lib/python3.6/site-packages/pandas/io/common.py\", line 149, in _stringify_path\n",
      "    from py.path import local as LocalPath\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 951, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 894, in _find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1157, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1126, in _get_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1090, in _path_importer_cache\n",
      "OSError: [Errno 5] Input/output error\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/miglesia/anaconda2/envs/my_pymc_env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/miglesia/anaconda2/envs/my_pymc_env/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/miglesia/anaconda2/envs/my_pymc_env/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/miglesia/anaconda2/envs/my_pymc_env/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/miglesia/anaconda2/envs/my_pymc_env/lib/python3.6/inspect.py\", line 1488, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/miglesia/anaconda2/envs/my_pymc_env/lib/python3.6/inspect.py\", line 1446, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/miglesia/anaconda2/envs/my_pymc_env/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/miglesia/anaconda2/envs/my_pymc_env/lib/python3.6/inspect.py\", line 725, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/home/miglesia/anaconda2/envs/my_pymc_env/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/home/miglesia/anaconda2/envs/my_pymc_env/lib/python3.6/posixpath.py\", line 376, in abspath\n",
      "    cwd = os.getcwd()\n",
      "OSError: [Errno 5] Input/output error\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 5] Input/output error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "## Cov matrices of some experiments\n",
    "R = 5\n",
    "scale = 'lin'\n",
    "sorted_ = False\n",
    "\n",
    "for i in [0, 1]:\n",
    "\n",
    "    ###########################################################\n",
    "\n",
    "\n",
    "    bs_base = pd.read_csv('./../../data/bootstraps/bs_base_m'+str(n)+'_q'+str(Q)+'_'+['X', 'M'][i]+'.csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "    bs_totl = pd.read_csv('./../../data/bootstraps/bs_totl_m'+str(n)+'_q'+str(Q)+'_'+['X', 'M'][i]+'.csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "\n",
    "    if sorted_:\n",
    "        bs_base = bs_base.loc[bs_base.parts == 'Q'].drop('parts', axis = 1)\n",
    "        bs_totl = bs_totl.loc[bs_totl.parts == 'Q'].drop('parts', axis = 1)\n",
    "    else:\n",
    "        bs_base = bs_base.loc[bs_base.parts == 'P'].drop('parts', axis = 1)\n",
    "        bs_totl = bs_totl.loc[bs_totl.parts == 'P'].drop('parts', axis = 1)\n",
    "\n",
    "    ### ADDITIVE DECOMPOSITION\n",
    "\n",
    "    bs_base = bs_base.groupby(level = ['s', 'q', 'm']).sum()  # drops the nq, it's not really grouping\n",
    "    bs_totl = bs_totl.groupby(level = ['s', 'q', 'm']).sum()  # drops the nq, it's not really grouping\n",
    "    fe_n = (bs_totl - bs_base)\n",
    "    fe = fe_n.groupby(level = ['s', 'q']).transform('median') \n",
    "    noise = fe_n - fe\n",
    "\n",
    "    ### MULTIPLICATIVE DECOMPOSITION\n",
    "\n",
    "    mul_fe_noise = bs_totl/bs_base\n",
    "    # Now, I use the bs to estimate disentangling fe from noise\n",
    "    mul_fe = mul_fe_noise.groupby(level = ['s', 'q']).transform('median') \n",
    "    mul_noise = mul_fe_noise/mul_fe\n",
    "\n",
    "    ###########################################################\n",
    "\n",
    "    fig, axs = plt.subplots(1, R, figsize = (4*R, 4))\n",
    "\n",
    "    for r in range(R):\n",
    "        ax = axs[r]\n",
    "    #     pick_m = np.random.choice(range(n))\n",
    "        pick_m = [3, 6, 2, 5, 8][r]\n",
    "\n",
    "        totl_ = bs_totl.loc[bs_totl.index.get_level_values('m') == pick_m].loc[0.5]\n",
    "        base_ = bs_base.loc[bs_base.index.get_level_values('m') == pick_m].loc[0.5]\n",
    "        fe_n_ = fe_n.loc[fe_n.index.get_level_values('m') == pick_m].loc[0.5]\n",
    "        noise_ = noise.loc[noise.index.get_level_values('m') == pick_m].loc[0.5]\n",
    "        fe_ = fe.loc[fe.index.get_level_values('m') == pick_m].loc[0.5]\n",
    "\n",
    "        # totl_ - base_ - fe_n_\n",
    "        C = pd.concat([base_.T, fe_.T, noise_.T], axis=1).cov()\n",
    "\n",
    "        scale = 'lin'\n",
    "\n",
    "        mat_data = C.values; \n",
    "        print('sum:'+str(mat_data.sum())); print('trace:'+str(np.diagonal(mat_data).sum()));\n",
    "\n",
    "        if scale == 'log': \n",
    "            c = 3\n",
    "        else:\n",
    "            c = -18\n",
    "\n",
    "        mat_data_sc = mat_data*10**(c)\n",
    "\n",
    "        im = ax.imshow(mat_data_sc, vmin = -5, vmax = 5, cmap = 'RdYlGn')\n",
    "\n",
    "        # Show all ticks...\n",
    "        ax.set_xticks(np.arange(0, 3*Q, 10))\n",
    "        ax.set_yticks(np.arange(0, 3*Q, 10))\n",
    "        # ... and label them with the respective list entries\n",
    "        # ax.set_xticklabels(range(1, 3*Q + 1))\n",
    "        # ax.set_yticklabels(range(1, 3*Q + 1))\n",
    "\n",
    "        ax.axhline(2*Q - .5, c = 'k')\n",
    "        ax.axvline(2*Q - .5, c = 'k')\n",
    "        ax.axhline(Q - .5, c = 'k')\n",
    "        ax.axvline(Q - .5, c = 'k')\n",
    "\n",
    "    # plt.savefig('./../../../WRITING/paper1_writing/figures/full_crosscov_'+scale+'.png')\n",
    "#     plt.savefig('./../../../WRITING/paper1_writing/figures/full_crosscov_exs_'+scale+'_'+['random', 'sorted'][sorted_]+'_'+['X', 'M'][i]+'.png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# logsales = np.log10(sales)\n",
    "# demlogsales = logsales.subtract(logsales.mean(1), axis = 0)\n",
    "\n",
    "# sizes = sales.sum(1)\n",
    "\n",
    "\n",
    "# Q = 10\n",
    "# parts = pd.cut(sizes.cumsum()/sizes.sum(), Q, labels = range(Q)).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from matplotlib.pyplot import Line2D\n",
    "\n",
    "P = 10\n",
    "s = 0.5\n",
    "df = pd.read_csv('./../../data/processed/ID_Y.csv')\n",
    "\n",
    "for i in [0, 1]:\n",
    "    # # df = pd.read_csv('./../../data/processed/.csv')\n",
    "    sales = df.loc[df.IMPORT == i].groupby(['ID', 'YEAR'])['VART'].sum().unstack()\n",
    "    sales = sales.loc[sales.sum(1).sort_values().index]\n",
    "    \n",
    "    \n",
    "    ###########################################################\n",
    "\n",
    "    bs_base = pd.read_csv('./../../data/bootstraps/bs_base_m'+str(n)+'_q'+str(Q)+'_'+['X', 'M'][i]+'.csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "    bs_totl = pd.read_csv('./../../data/bootstraps/bs_totl_m'+str(n)+'_q'+str(Q)+'_'+['X', 'M'][i]+'.csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "\n",
    "    if sorted_:\n",
    "        bs_base = bs_base.loc[bs_base.parts == 'Q'].drop('parts', axis = 1)\n",
    "        bs_totl = bs_totl.loc[bs_totl.parts == 'Q'].drop('parts', axis = 1)\n",
    "    else:\n",
    "        bs_base = bs_base.loc[bs_base.parts == 'P'].drop('parts', axis = 1)\n",
    "        bs_totl = bs_totl.loc[bs_totl.parts == 'P'].drop('parts', axis = 1)\n",
    "\n",
    "    ### ADDITIVE DECOMPOSITION\n",
    "\n",
    "    bs_base = bs_base.groupby(level = ['s', 'q', 'm']).sum()  # drops the nq, it's not really grouping\n",
    "    bs_totl = bs_totl.groupby(level = ['s', 'q', 'm']).sum()  # drops the nq, it's not really grouping\n",
    "    fe_n = (bs_totl - bs_base)\n",
    "    fe = fe_n.groupby(level = ['s', 'q']).transform('median') \n",
    "    noise = fe_n - fe\n",
    "\n",
    "    ### MULTIPLICATIVE DECOMPOSITION\n",
    "\n",
    "    mul_fe_noise = bs_totl/bs_base\n",
    "    # Now, I use the bs to estimate disentangling fe from noise\n",
    "    mul_fe = mul_fe_noise.groupby(level = ['s', 'q']).transform('median') \n",
    "    mul_noise = mul_fe_noise/mul_fe\n",
    "\n",
    "    ###########################################################\n",
    "    \n",
    "    \n",
    "\n",
    "    print(['EXPORTS', 'IMPORTS'][i])\n",
    "    fig, axs = plt.subplots(2, 3, figsize = (15, 7))\n",
    "    ylim_scale = 1.5e11\n",
    "\n",
    "    ax = axs[0][0]\n",
    "    ax.set_title(r'Additive Base $B_{pt}$')\n",
    "    deviations_info = bs_base.groupby(level = [0, 1]).mean().loc[s]\n",
    "    deviations_info = P*deviations_info.subtract(deviations_info.mean(1), axis = 0)  # This is to subtract Sq from Bq\n",
    "\n",
    "    deviations_info.T.plot(c = '.5', alpha = .35, legend = False, ax = ax)\n",
    "    deviations_info.mean().plot(marker = 's', lw = 0, ax = ax, c = '#2ca02c')\n",
    "    ax.set_ylim(-ylim_scale, ylim_scale)\n",
    "    ax.axhline(0, lw = 1, c = '.5', linestyle = '--')\n",
    "    ax.get_yticklabels()[4].set_backgroundcolor('yellow')\n",
    "\n",
    "\n",
    "    ax = axs[0][1]\n",
    "    ax.set_title(r'Additive Mean deviations $M_{pt}$')\n",
    "\n",
    "    ## Full magnitude\n",
    "    deviations_info = P*fe.groupby(level = [0, 1]).mean().loc[s]\n",
    "    ax.axhline(deviations_info.mean().mean(), lw = 1, c = '.5', linestyle = ':')\n",
    "    deviations_info.T.plot(c = '.5', alpha = .35, legend = False, ax = ax)\n",
    "    deviations_info.mean().plot(marker = 's', lw = 0, c = '#d62728', ax = ax)\n",
    "\n",
    "    ## Half magnitude\n",
    "    deviations_info = P*fe.groupby(level = [0, 1]).mean().loc[0.25]\n",
    "    ax.axhline(deviations_info.mean().mean(), lw = 1, c = '.5', linestyle = ':')\n",
    "    deviations_info.T.plot(c = '.5', alpha = .1, legend = False, ax = ax)\n",
    "    deviations_info.mean().plot(marker = 's', lw = 0, c = '#d62728', ax = ax, alpha = .3)\n",
    "\n",
    "    ax.axhline(0, lw = 1, c = '.5', linestyle = '--')\n",
    "    ax.set_ylim(-ylim_scale, ylim_scale)\n",
    "    ax.get_yticklabels()[4].set_backgroundcolor('yellow')\n",
    "    \n",
    "    \n",
    "    legend_elements = [Line2D([0], [0], marker='s', color='#d62728', label='full micro shocks', lw = 0),\n",
    "                   Line2D([0], [0], marker='s', color='#d62728', label='half micro shocks', lw = 0, alpha = .3)]\n",
    "    ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "\n",
    "    ax = axs[0][2]\n",
    "    ax.set_title(r'Total Exports time series - mean')\n",
    "    deviations_info = sales.sum().subtract(sales.sum().mean(), axis = 0)/2\n",
    "    deviations_info.plot(marker = 's', lw = 1, ax = ax, legend = False)\n",
    "    ax.axhline(0, lw = 1, c = '.5', linestyle = '--')\n",
    "    ax.set_ylim(-ylim_scale, ylim_scale)\n",
    "    ax.get_yticklabels()[4].set_backgroundcolor('yellow')\n",
    "    \n",
    "    ###########################################################    \n",
    "    ylim_scale = 2\n",
    "    \n",
    "    ax = axs[1][0]\n",
    "    ax.set_title(r'Multiplicative Base $b_{pt}$')\n",
    "    deviations_info = bs_base.groupby(level = [0, 1]).mean().loc[s]\n",
    "    deviations_info = deviations_info.div(deviations_info.mean(1), axis = 0)  # This is to subtract Sq from Bq\n",
    "    deviations_info.T.plot(c = '.5', alpha = .35, legend = False, ax = ax)\n",
    "    deviations_info.mean().plot(marker = 's', lw = 0, ax = ax, c = '#2ca02c')\n",
    "    # ax.axhline(deviations_info.mean().mean(), lw = 4, c = 'r', linestyle = ':') # mean is on 1\n",
    "    ax.axhline(1, lw = 1, c = '.5', linestyle = '--')\n",
    "    ax.set_ylim(1/ylim_scale, ylim_scale)\n",
    "    ax.get_yticklabels()[3].set_backgroundcolor('yellow')\n",
    "\n",
    "\n",
    "    ax = axs[1][1]\n",
    "    ax.set_title(r'Multiplicative Mean deviations $m_{pt}$')\n",
    "\n",
    "    ## Full magnitude\n",
    "    deviations_info = mul_fe.groupby(level = [0, 1]).mean().loc[s]\n",
    "    deviations_info.T.plot(c = '.5', alpha = .35, legend = False, ax = ax)\n",
    "    pts1 = deviations_info.mean().plot(marker = 's', lw = 0, ax = ax, c = '#d62728')\n",
    "    ax.axhline(deviations_info.mean().mean(), lw = 1, c = '.5', linestyle = ':')\n",
    "\n",
    "    ## Half magnitude\n",
    "    deviations_info = mul_fe.groupby(level = [0, 1]).mean().loc[0.25]\n",
    "    deviations_info.T.plot(c = '.5', alpha = .1, legend = False, ax = ax)\n",
    "    pts2 = deviations_info.mean().plot(marker = 's', lw = 0, ax = ax, c = '#d62728', alpha = .3)\n",
    "    ax.axhline(deviations_info.mean().mean(), lw = 1, c = '.5', linestyle = ':')\n",
    "\n",
    "    ax.axhline(1, lw = 1, c = '.5', linestyle = '--')\n",
    "    ax.set_ylim(1/ylim_scale, ylim_scale)\n",
    "    ax.get_yticklabels()[3].set_backgroundcolor('yellow')\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], marker='s', color='#d62728', label='full micro shocks', lw = 0),\n",
    "                   Line2D([0], [0], marker='s', color='#d62728', label='half micro shocks', lw = 0, alpha = .3)]\n",
    "    ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "\n",
    "    ax = axs[1][2]\n",
    "    ax.set_title(r'Total Exports time series / mean')\n",
    "    (sales.sum()/sales.sum().mean()).plot(marker = 's', lw = 1, ax = ax, legend = False)\n",
    "    ax.axhline(1, lw = 1, c = '.5', linestyle = '--')\n",
    "    ax.set_ylim(1/ylim_scale, ylim_scale)\n",
    "    ax.get_yticklabels()[3].set_backgroundcolor('yellow')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./../../../WRITING/paper1_writing/figures/components_ts_'+['X', 'M'][i]+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Possible Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# import numpy as np\n",
    "\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 100\n",
    "# Q = 10\n",
    "\n",
    "# # non-anonymous function\n",
    "# def pct10 (x):\n",
    "#     return np.percentile(x, q=10)\n",
    "\n",
    "# def pct25 (x): \n",
    "#     return np.percentile(x, q=25)\n",
    "\n",
    "# def pct75 (x): \n",
    "#     return np.percentile(x, q=75)\n",
    "\n",
    "# def pct90 (x): \n",
    "#     return np.percentile(x, q=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #### LOAD DATA\n",
    "\n",
    "# for size_sorting in [True, False]:\n",
    "# #     bs_base = pd.read_csv('./../../data/bootstraps/bs_base_'+['random', 'sorted'][size_sorting]+'_m'+str(n)+'_q'+str(Q)+'csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "# #     bs_totl = pd.read_csv('./../../data/bootstraps/bs_totl_'+['random', 'sorted'][size_sorting]+'_m'+str(n)+'_q'+str(Q)+'csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "    \n",
    "#     bs_base = pd.read_csv('./../../data/bootstraps/bs_base_m'+str(n)+'_q'+str(Q)+'csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "#     bs_totl = pd.read_csv('./../../data/bootstraps/bs_totl_m'+str(n)+'_q'+str(Q)+'csv', index_col=['q', 'm', 's', 'nqs'])\n",
    "#     bs_base = bs_base.loc[bs_base.parts == ['P', 'Q'][size_sorting]].drop('parts', axis = 1)\n",
    "#     bs_totl = bs_totl.loc[bs_totl.parts == ['P', 'Q'][size_sorting]].drop('parts', axis = 1)\n",
    "\n",
    "#     ### ADDITIVE DECOMPOSITION\n",
    "\n",
    "#     bs_base = bs_base.groupby(level = ['s', 'q', 'm']).sum()  # drops the nq, it's not really grouping\n",
    "#     bs_totl = bs_totl.groupby(level = ['s', 'q', 'm']).sum()  # drops the nq, it's not really grouping\n",
    "#     fe_n = (bs_totl - bs_base)\n",
    "#     fe = fe_n.groupby(level = ['s', 'q']).transform('median') \n",
    "#     noise = fe_n - fe\n",
    "\n",
    "#     ### MULTIPLICATIVE DECOMPOSITION\n",
    "\n",
    "#     mul_fe_noise = bs_totl/bs_base\n",
    "#     # Now, I use the bs to estimate disentangling fe from noise\n",
    "#     mul_fe = mul_fe_noise.groupby(level = ['s', 'q']).transform('median') \n",
    "#     mul_noise = mul_fe_noise/mul_fe\n",
    "\n",
    "#     # Important: they fulfill\n",
    "#     # bs_totl - bs_base*mul_fe*mul_noise == 0\n",
    "#     # np.log10(bs_totl) - np.log10(bs_base) - np.log10(mul_fe) - np.log10(mul_noise) == 0\n",
    "\n",
    "#     for linear in [True, False]:\n",
    "\n",
    "#         if linear:\n",
    "#             BASE = bs_base\n",
    "#             FE = fe\n",
    "#             NOISE = noise\n",
    "#         else:\n",
    "#             BASE = np.log10(bs_base) # bs_base\n",
    "#             FE = np.log10(mul_fe) # fe\n",
    "#             NOISE = np.log10(mul_noise)# noise\n",
    "\n",
    "\n",
    "#         result_list = []\n",
    "#         obs_parts_cross_cov_list = []\n",
    "\n",
    "#         for m in range(n):\n",
    "#             if m%50 == 0: print(m)\n",
    "#             for s in [.02, .05, .1, .25, .5]:\n",
    "#                 base_m = BASE.loc[(BASE.index.get_level_values('m') == m) & (BASE.index.get_level_values('s') == s)]\n",
    "#                 fe_m = FE.loc[(FE.index.get_level_values('m') == m) & (FE.index.get_level_values('s') == s)]\n",
    "#                 n_m = NOISE.loc[(NOISE.index.get_level_values('m') == m) & (NOISE.index.get_level_values('s') == s)]\n",
    "\n",
    "#                 obs_s_m = pd.concat([base_m, fe_m, n_m])\n",
    "#                 obs_s_m.index = pd.MultiIndex.from_product([['base', 'FE', 'noise'],range(Q)], names = ['comp', 'part'])\n",
    "\n",
    "#                 cov = obs_s_m.T.cov() # shape: 3Q x 3Q\n",
    "\n",
    "#                 ####### OUTCOME 1\n",
    "#     #           Save observed cross covariance of the parts\n",
    "\n",
    "# #                 obs_parts_cross_cov = cov.loc['noise'][['noise']].stack(); obs_parts_cross_cov.index.names = ['partA', 'partB']; obs_parts_cross_cov.columns = ['cov']\n",
    "# #                 obs_parts_cross_cov['m'] = m; obs_parts_cross_cov['s'] = s\n",
    "# #                 obs_parts_cross_cov = obs_parts_cross_cov.reset_index().set_index(['s', 'm', 'partA', 'partB'])\n",
    "\n",
    "# #                 obs_parts_cross_cov_list += [obs_parts_cross_cov]\n",
    "\n",
    "#                 obs_parts_cross_cov = pd.DataFrame(cov.stack([0, 1]), columns = ['cov'])\n",
    "#                 obs_parts_cross_cov.index.names = ['compA', 'partA', 'compB', 'partB']\n",
    "#                 obs_parts_cross_cov['m'] = m; obs_parts_cross_cov['s'] = s\n",
    "#                 obs_parts_cross_cov = obs_parts_cross_cov.reset_index().set_index(['s', 'm', 'compA', 'partA', 'compB', 'partB'])\n",
    "\n",
    "#                 obs_parts_cross_cov_list += [obs_parts_cross_cov]\n",
    "\n",
    "\n",
    "#                 # np.log10(abs(obs_s_m.T.cov())).round(1)\n",
    "\n",
    "#                 ####### OUTCOME 2\n",
    "#                 # Characterize diagonal and offdiagonal for all 3x3 cross components\n",
    "\n",
    "#                 ## Make a mask for i = j\n",
    "#                 ## Groupby the other level and (we need to sum) but possibly describe.\n",
    "#                 part_index = np.array(3*Q*[cov.index.get_level_values('part').values])\n",
    "\n",
    "#                 ## Mask for off diagonals\n",
    "#                 part_diag_mask = np.equal(part_index, part_index.T)\n",
    "#                 pd_diag_mask = pd.DataFrame(part_diag_mask, index = cov.index, columns = cov.columns)\n",
    "\n",
    "\n",
    "#                 diag = cov[pd_diag_mask].stack([-2, -1]).groupby(level = [0, 2]).agg(['count', np.mean, np.sum, np.std, \n",
    "#                                                                                       pct10, pct25, pct75, pct90])\n",
    "#                 diag = pd.concat([diag], keys=['diag'], names=['diag'], axis = 0)\n",
    "\n",
    "#                 offdiag = cov[~pd_diag_mask].stack([-2, -1]).groupby(level = [0, 2]).agg(['count', np.mean, np.sum, np.std,\n",
    "#                                                                                       pct10, pct25, pct75, pct90])\n",
    "#                 offdiag = pd.concat([offdiag], keys=['offdiag'], names=['diag'], axis = 0)\n",
    "\n",
    "\n",
    "#                 result_m = pd.concat([diag, offdiag])\n",
    "#                 result_m = pd.concat([result_m], keys=[m], names=['m'], axis = 0)\n",
    "#                 result_m = pd.concat([result_m], keys=[s], names=['s'], axis = 0)\n",
    "#                 result_m.index.names = ['s', 'm', 'diag', 'compA', 'compB']\n",
    "\n",
    "#                 result_list += [result_m]\n",
    "                \n",
    "#         result_varname = '_'.join(['cov_elements_desc', ['random', 'sorted'][size_sorting], ['log', 'lin'][linear]]); print(result_varname)\n",
    "#         globals()[result_varname] = pd.concat(result_list)\n",
    "        \n",
    "        \n",
    "#         result_varname = '_'.join(['parts_cross_cov', ['random', 'sorted'][size_sorting], ['log', 'lin'][linear]]); print(result_varname)\n",
    "#         globals()[result_varname] = pd.concat(obs_parts_cross_cov_list)\n",
    "                \n",
    "# #         if linear:\n",
    "# #             result_lin = pd.concat(result_list)\n",
    "# #             result_parts_cross_cov_lin = pd.concat(obs_parts_cross_cov_list) ## adapt\n",
    "# #         else:\n",
    "# #             result_log = pd.concat(result_list)\n",
    "# #             result_parts_cross_cov_log = pd.concat(obs_parts_cross_cov_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cov matrix elements info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, df in enumerate([cov_elements_desc_sorted_lin, parts_cross_cov_sorted_lin, cov_elements_desc_sorted_log, \n",
    "#            parts_cross_cov_sorted_log, cov_elements_desc_random_lin, parts_cross_cov_random_lin, \n",
    "#                        cov_elements_desc_random_log, parts_cross_cov_random_log]):\n",
    "#     name = ['cov_elements_desc_sorted_lin', 'parts_cross_cov_sorted_lin', 'cov_elements_desc_sorted_log', \n",
    "#            'parts_cross_cov_sorted_log', 'cov_elements_desc_random_lin', 'parts_cross_cov_random_lin', \n",
    "#            'cov_elements_desc_random_log', 'parts_cross_cov_random_log'][i];\n",
    "    \n",
    "#     df.to_csv('./../../data/bootstraps/'+name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.options.display.max_columns = 99\n",
    "# pd.options.display.max_rows = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs_base + fe + n - bs_totl == 0 . OK\n",
    "\n",
    "# I want sum of cross covariance terms for components of the same m\n",
    "\n",
    "# there are 6 varieties of combination (b, fe), (fe, n), (n, b), bb nn fefe, on diagonal (i = j) and off diagonal.\n",
    "\n",
    "# Not sure what is a practical way to slice data and cov matrices...\n",
    "# But anyway everything I want is just here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_log_scaled = result_log.copy()\n",
    "# result_log_scaled[['mean','sum','std']] = component_scale_factor*result_log[['mean','sum','std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_scale_factor = (((Sk/2).mean(1)*np.log(10))**2).mean()   ## This is (Sq*ln10)**2, but Sq/2 because of BS\n",
    "component_scale_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cov elements info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0, 1]:\n",
    "    for size_sorting in [True, False]:\n",
    "        for linear in [True, False]:\n",
    "            name = '_'.join(['cov_elements_desc', ['random', 'sorted'][size_sorting], ['log', 'lin'][linear], ['X', 'M'][i]]); print(name)\n",
    "            globals()[name] = pd.read_csv('./../../data/bootstraps/'+name+'.csv')\n",
    "            \n",
    "            name = '_'.join(['parts_cross_cov', ['random', 'sorted'][size_sorting], ['log', 'lin'][linear], ['X', 'M'][i]]); print(name)\n",
    "            globals()[name] = pd.read_csv('./../../data/bootstraps/'+name+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Residuals\n",
    "(Difference of cov elements from mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here just merge observed elements to mean values.\n",
    "# then, columns can be subtracted.\n",
    "\n",
    "# Mean of means. Can do it because for all m there are equal number of elements in each group.\n",
    "\n",
    "df_desc = cov_elements_desc_sorted_lin_X\n",
    "df_obs = parts_cross_cov_sorted_lin_X\n",
    "\n",
    "mean_values = df_desc.groupby(['s', 'diag', 'compA', 'compB'])['mean'].mean().reset_index()\n",
    "\n",
    "# the sigma i should be the sqrt of the diagonal terms.\n",
    "sigmas = obs_mean.loc[(obs_mean.compA == obs_mean.compB) & (obs_mean.partA == obs_mean.partB)]\n",
    "sigmas['std'] = np.sqrt(sigmas['cov'])\n",
    "sigmas = sigmas.groupby(['s', 'compA','compB', 'diag'])['std'].mean().reset_index()\n",
    "\n",
    "\n",
    "# len(df_desc)/9/2/100/5 == 1 # 9 blocks, diag-off diag, Q2, s\n",
    "\n",
    "df_obs['diag'] = (df_obs.partA == df_obs.partB).map({True: 'diag', False: 'offdiag'})\n",
    "\n",
    "df_obs.head()\n",
    "\n",
    "obs_mean = df_obs.merge(mean_values[['s', 'compA', 'compB', 'diag', 'mean']])\n",
    "obs_mean['diff'] = obs_mean['cov'] - obs_mean['mean']\n",
    "\n",
    "obs_mean_sigmas = obs_mean.merge(sigmas[['s', 'compA', 'std']], on = ['s', 'compA']).merge(sigmas[['s', 'compB', 'std']], on = ['s', 'compB'])\n",
    "obs_mean_sigmas = obs_mean_sigmas.rename(columns = {'std_x': 'std_A', 'std_y': 'std_B'})\n",
    "\n",
    "obs_mean_sigmas['cov_ee'] = obs_mean_sigmas['diff']/ (obs_mean_sigmas['std_A'] * obs_mean_sigmas['std_B'])\n",
    "\n",
    "# sq_devs = obs_mean_sigmas.loc[obs_mean_sigmas.s == 0.5].groupby(['compA','partA','compB','partB']).apply(lambda x: ((x['diff']/ x['std_A']/ x['std_B'])**2).mean())\n",
    "# plt.imshow(sq_devs.unstack([0, 1]).values)\n",
    "# sq_devs.unstack([0, 1]).round(1)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.matshow(np.log10(sq_devs.unstack([0, 1]).values))\n",
    "fig, axs = plt.subplots(3,3, figsize = (12, 12))\n",
    "    \n",
    "for i, compA in enumerate(['base', 'FE', 'noise']):\n",
    "    for j, compB in enumerate(['base', 'FE', 'noise']):\n",
    "        ax = axs[i][j]\n",
    "        if i <= j:\n",
    "            df = obs_mean_sigmas\n",
    "            df = df.loc[df.s == 0.5]\n",
    "            df = df.loc[(df.compA == compA) & (df.compB == compB)]\n",
    "\n",
    "            for d in ['diag', 'offdiag']:\n",
    "#                 print(len(df.loc[df.diag == d].cov_ee))\n",
    "                ax.hist(df.loc[df.diag == d].cov_ee, np.arange(-2, 2, .15), normed = True, alpha = .5)\n",
    "#                 ax.set_yscale('log')\n",
    "\n",
    "        else:\n",
    "            ax.set_axis_off()\n",
    "                \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See mean values of Cov matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Older plots. Legacy.\n",
    "\n",
    "# result = cov_elements_desc_random_lin\n",
    "# plot_data = result.reset_index().groupby(['compA', 'compB', 'diag', 's']).mean().drop('m', axis = 1).reset_index()\n",
    "\n",
    "# fig, axs = plt.subplots(3,3, figsize = (12, 12))\n",
    "# # plot_data = result.reset_index().groupby(['compA', 'compB', 'diag', 's']).mean().drop('m', axis = 1).reset_index()\n",
    "# plot_data['i'] = plot_data.compA.map({'FE': 1, 'base': 0, 'noise': 2})\n",
    "# plot_data['j'] = plot_data.compB.map({'FE': 1, 'base': 0, 'noise': 2})\n",
    "\n",
    "# # box titles \n",
    "# titles = np.array([[r'$cov(B_{it}, B_{jt})$', r'$cov(B_{it}, M_{jt})$', r'$cov(B_{it}, E_{jt})$'], ['', r'$cov(M_{it}, M_{jt})$', r'$cov(M_{it}, E_{jt})$'], ['','' , r'$cov(E_{it}, E_{jt})$']])\n",
    "\n",
    "# for i in range(3):\n",
    "#     for j in range(3):\n",
    "#         ax = axs[i][j]\n",
    "#         if i <= j:\n",
    "#             ax.set_title(titles[i][j])\n",
    "#             slice_data = plot_data.loc[(plot_data.i == i) & (plot_data.j == j)].reset_index(drop = True)\n",
    "\n",
    "#             diag_data = slice_data.loc[slice_data.diag == 'diag']\n",
    "#             p1 = ax.bar(diag_data.index, diag_data['mean'], yerr = diag_data['std'])\n",
    "\n",
    "#             offdiag_data = slice_data.loc[slice_data.diag == 'offdiag']\n",
    "#             p2 = ax.bar(offdiag_data.index + 1, offdiag_data['mean'], yerr = offdiag_data['std'])\n",
    "\n",
    "#             ax.set_ylim(-3e18, 1e19)\n",
    "            \n",
    "#             labels = []\n",
    "#             ax.set_xticks(list(diag_data.index) + list(offdiag_data.index + 1))\n",
    "#             ax.set_xticklabels(slice_data.s.values, rotation=45, ha = 'right')\n",
    "            \n",
    "#             ax.legend((p1[0], p2[0]), (r'$i = j$', r'$i \\neq j$'))\n",
    "            \n",
    "#             ax.axvline(5, c = '.5', lw = 1)\n",
    "            \n",
    "#             ax.set_xlabel('std of firm sales '+r'$(\\sigma)$')\n",
    "\n",
    "#         else:\n",
    "#             ax.set_axis_off()\n",
    "            \n",
    "# plt.tight_layout()\n",
    "# # plt.savefig('./../../../WRITING/paper1_writing/figures/components_crosscov.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blue, yellow colors.\n",
    "# '#1f77b4',\n",
    "#  '#ff7f0e',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     16
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in [0, 1]:\n",
    "    for k, result in enumerate([cov_elements_desc_random_lin_X, cov_elements_desc_sorted_lin_X,\n",
    "                               cov_elements_desc_random_log_X, cov_elements_desc_sorted_log_X]): # arreglar X, M\n",
    "        print(['Random Parts. Linear var', 'Quantile Parts. Linear var',\n",
    "              'Random Parts. Log var', 'Quantile Parts. Log var'][k])\n",
    "\n",
    "        plot_data = result.reset_index().groupby(['compA', 'compB', 'diag', 's']).mean().drop('m', axis = 1).reset_index()\n",
    "\n",
    "        fig, axs = plt.subplots(3,3, figsize = (12, 12))\n",
    "        # plot_data = result.reset_index().groupby(['compA', 'compB', 'diag', 's']).mean().drop('m', axis = 1).reset_index()\n",
    "        plot_data['i'] = plot_data.compA.map({'FE': 1, 'base': 0, 'noise': 2})\n",
    "        plot_data['j'] = plot_data.compB.map({'FE': 1, 'base': 0, 'noise': 2})\n",
    "\n",
    "        # box titles \n",
    "        titles = np.array([[r'$cov(B_{it}, B_{jt})$', r'$cov(B_{it}, M_{jt})$', r'$cov(B_{it}, E_{jt})$'], ['', r'$cov(M_{it}, M_{jt})$', r'$cov(M_{it}, E_{jt})$'], ['','' , r'$cov(E_{it}, E_{jt})$']])\n",
    "\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                ax = axs[i][j]\n",
    "                if i <= j:\n",
    "                    ax.set_title(titles[i][j])\n",
    "                    slice_data = plot_data.loc[(plot_data.i == i) & (plot_data.j == j)].reset_index(drop = True)\n",
    "\n",
    "                    diag_data = slice_data.loc[slice_data.diag == 'diag']\n",
    "        #             p1 = ax.bar(diag_data.index, diag_data['mean'], yerr = diag_data['std'])\n",
    "\n",
    "                    x = diag_data.index; y = diag_data['mean'];\n",
    "                    p11 = ax.plot(x, y, marker = '.', lw = 0, c = 'k')\n",
    "                    p12 = ax.bar(x, bottom = diag_data['pct25'], height = diag_data['pct75'] - diag_data['pct25'], width = .3, alpha = .5, color = '#1f77b4')\n",
    "                    p13 = ax.bar(x, bottom = diag_data['pct10'], height = diag_data['pct90'] - diag_data['pct10'], width = .15, alpha = .5, color = '#1f77b4')\n",
    "\n",
    "                    offdiag_data = slice_data.loc[slice_data.diag == 'offdiag']\n",
    "        #             p2 = ax.bar(offdiag_data.index + 1, offdiag_data['mean'], yerr = offdiag_data['std'])\n",
    "                    x = offdiag_data.index + 1; y = offdiag_data['mean'];\n",
    "                    p21 = ax.plot(x, y, marker = '.', lw = 0, c = 'k')\n",
    "                    p22 = ax.bar(x, bottom = offdiag_data['pct25'], height = offdiag_data['pct75'] - offdiag_data['pct25'], width = .3, alpha = .5, color = '#ff7f0e')\n",
    "                    p33 = ax.bar(x, bottom = offdiag_data['pct10'], height = offdiag_data['pct90'] - offdiag_data['pct10'], width = .15, alpha = .5, color = '#ff7f0e')\n",
    "\n",
    "                    if k >= 2: # Log variances\n",
    "                        ax.set_ylim(-0.0025, 0.0125)\n",
    "                    else: # Linear variances\n",
    "                        ax.set_ylim(-.5e19, 2.5e19)\n",
    "\n",
    "                    ax.axhline(0, c = '.5', alpha = .6)\n",
    "\n",
    "                    labels = []\n",
    "                    ax.set_xticks(list(diag_data.index) + list(offdiag_data.index + 1))\n",
    "                    ax.set_xticklabels(slice_data.s.values, rotation=45, ha = 'right')\n",
    "\n",
    "                    ax.legend((p11[0], p12[0], p22[0]), ('mean', r'$i = j$', r'$i \\neq j$'))#, , r'$i \\neq j$'))\n",
    "\n",
    "                    ax.axvline(5, c = '.5', lw = 1)\n",
    "\n",
    "                    ax.set_xlabel('var of firm sales '+r'$(\\sigma^2)$')\n",
    "\n",
    "                else:\n",
    "                    ax.set_axis_off()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filetag = ['P_lin', 'Q_lin', 'P_log', 'Q_log'][k]\n",
    "        plt.savefig('./../../../WRITING/paper1_writing/figures/components_crosscov_'+filetag+'_'+['X', 'M'][i]+'.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross covariances of parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # parts_cross_cov_random_log.groupby(['s', 'partA', 'partB']).agg(['mean', 'median', 'std'])\n",
    "\n",
    "# m_pick = np.random.choice(range(n))\n",
    "\n",
    "# cov_vals = parts_cross_cov_random_log\n",
    "\n",
    "# cov_vals = cov_vals.loc[(cov_vals.m == m_pick) & (cov_vals.s == .5)]\n",
    "\n",
    "# cov_vals = cov_vals.set_index(['partA', 'partB'])[['cov']].unstack()\n",
    "\n",
    "# pd.DataFrame(np.flip(cov_vals.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in ['cov_elements_desc_sorted_lin', 'parts_cross_cov_sorted_lin', 'cov_elements_desc_sorted_log', \n",
    "           'parts_cross_cov_sorted_log', 'cov_elements_desc_random_lin', 'parts_cross_cov_random_lin',\n",
    "            'cov_elements_desc_random_log', 'parts_cross_cov_random_log']:\n",
    "    globals()[name] = pd.read_csv('./../../data/bootstraps/'+name+'.csv')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parts_cross_cov_random_log['partA'] = Q - parts_cross_cov_random_log['partA']\n",
    "# parts_cross_cov_random_log['partB'] = Q - parts_cross_cov_random_log['partB']\n",
    "\n",
    "# parts_cross_cov_sorted_log['partA'] = Q - parts_cross_cov_sorted_log['partA']\n",
    "# parts_cross_cov_sorted_log['partB'] = Q - parts_cross_cov_sorted_log['partB']\n",
    "\n",
    "# parts_cross_cov_random_lin['partA'] = Q - parts_cross_cov_random_lin['partA']\n",
    "# parts_cross_cov_random_lin['partB'] = Q - parts_cross_cov_random_lin['partB']\n",
    "\n",
    "# parts_cross_cov_sorted_lin['partA'] = Q - parts_cross_cov_sorted_lin['partA']\n",
    "# parts_cross_cov_sorted_lin['partB'] = Q - parts_cross_cov_sorted_lin['partB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[0.5]['cov']['median'].unstack([-2, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for scale in ['lin', 'log']:\n",
    "\n",
    "    if scale == 'log':\n",
    "        rand = parts_cross_cov_random_log.groupby(['s', 'compA', 'partA', 'compB', 'partB']).agg(['mean', 'median', 'std'])\n",
    "        sort = parts_cross_cov_sorted_log.groupby(['s', 'compA', 'partA', 'compB', 'partB']).agg(['mean', 'median', 'std'])\n",
    "    else:\n",
    "        rand = parts_cross_cov_random_lin.groupby(['s', 'compA', 'partA', 'compB', 'partB']).agg(['mean', 'median', 'std'])\n",
    "        sort = parts_cross_cov_sorted_lin.groupby(['s', 'compA', 'partA', 'compB', 'partB']).agg(['mean', 'median', 'std'])\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize = (15, 7))\n",
    "\n",
    "    for i, df in enumerate([rand, sort]):\n",
    "        ax = axs[i]\n",
    "        mat_data = df.loc[0.5]['cov']['mean'].unstack([-2, -1])\n",
    "        mat_data = mat_data.reindex(['noise', 'FE', 'base'], level = 0).reindex(['noise', 'FE', 'base'], level = 0, axis = 1) # Custom shuffle of components\n",
    "        mat_data = np.flip(mat_data.values);# Flip for desired order of quantiles\n",
    "        \n",
    "        print('sum:'+str(mat_data.sum())); print('trace:'+str(np.diagonal(mat_data).sum()));\n",
    "               \n",
    "        std_data = df.loc[0.5]['cov']['std'].unstack([-2, -1])\n",
    "        std_data = std_data.reindex(['noise', 'FE', 'base'], level = 0).reindex(['noise', 'FE', 'base'], level = 0, axis = 1) # Custom shuffle of components\n",
    "        std_data = np.flip(std_data.values);# Flip for desired order of quantiles\n",
    "        \n",
    "        avg_magnitude = np.log10(abs(mat_data).mean()).round(); #c = int(-avg_magnitude)\n",
    "\n",
    "        if scale == 'log': \n",
    "            c = 3\n",
    "        else:\n",
    "            c = -18\n",
    "        \n",
    "        mat_data_sc = mat_data*10**(c)\n",
    "        std_data_sc = std_data*10**(c)\n",
    "\n",
    "#         display(mat_data_sc)\n",
    "        im = ax.imshow(mat_data_sc, vmin = -4, vmax = 4, cmap = 'RdYlGn')\n",
    "\n",
    "#         # Show all ticks...\n",
    "#         ax.set_xticks(range(Q))\n",
    "#         ax.set_yticks(range(Q))\n",
    "#         # ... and label them with the respective list entries\n",
    "#         ax.set_xticklabels(range(1, Q + 1))\n",
    "#         ax.set_yticklabels(range(1, Q + 1))\n",
    "\n",
    "        # Anotate cov labels  \n",
    "        if scale == 'lin':\n",
    "            for c1, x in enumerate(range(3)):\n",
    "                for c2, y in enumerate(range(3)):\n",
    "                    x = [4.5, 14.5, 24.5][c1]\n",
    "                    y = [4.5, 14.5, 24.5][c2]\n",
    "                    if x>=y:\n",
    "                        text = ax.text(x, y, r'$ cov('+['B_i', 'M_i', 'E_i'][c1]+', '+['B_j', 'M_j', 'E_j'][c2]+')$',\n",
    "                                           ha=\"center\", va=\"center\", color=\"k\", fontsize = 16)\n",
    "                    \n",
    "        if scale == 'log':\n",
    "            for c1, x in enumerate(range(3)):\n",
    "                for c2, y in enumerate(range(3)):\n",
    "                    x = [4.5, 14.5, 24.5][c1]\n",
    "                    y = [4.5, 14.5, 24.5][c2]\n",
    "                    if x>=y:\n",
    "                        text = ax.text(x, y, r'$ cov('+['b_i', 'm_i', '\\sigma_i \\epsilon_i'][c1]+', '+['b_j', 'm_j', '\\sigma_j \\epsilon_j'][c2]+')$',\n",
    "                                           ha=\"center\", va=\"center\", color=\"k\", fontsize = 16)\n",
    "            \n",
    "                \n",
    "\n",
    "        ax.set_title(['Random Parts. ', 'Quantile Parts. '][i] + \"Cross covariances (\"+ scale + ')', fontsize = 14)\n",
    "        \n",
    "        # Block lines\n",
    "        ax.axhline(2*Q - .5, c = 'k')\n",
    "        ax.axvline(2*Q - .5, c = 'k')\n",
    "        ax.axhline(Q - .5, c = 'k')\n",
    "        ax.axvline(Q - .5, c = 'k')\n",
    "\n",
    "    \n",
    "    fig.tight_layout()\n",
    "\n",
    "#     plt.savefig('./../../../WRITING/paper1_writing/figures/parts_crosscov_'+scale+'.png')\n",
    "    plt.savefig('./../../../WRITING/paper1_writing/figures/full_crosscov_'+scale+'.png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r'$ cov('+['B_i', 'M_i', 'E_i'][c1]+', '+['B_j', 'M_j', 'E_j'][c2]+')$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for scale in ['lin', 'log']:\n",
    "\n",
    "    if scale == 'log':\n",
    "        rand = parts_cross_cov_random_log.groupby(['s', 'partA', 'partB']).agg(['mean', 'median', 'std'])\n",
    "        sort = parts_cross_cov_sorted_log.groupby(['s', 'partA', 'partB']).agg(['mean', 'median', 'std'])\n",
    "    else:\n",
    "        rand = parts_cross_cov_random_lin.groupby(['s', 'partA', 'partB']).agg(['mean', 'median', 'std'])\n",
    "        sort = parts_cross_cov_sorted_lin.groupby(['s', 'partA', 'partB']).agg(['mean', 'median', 'std'])\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize = (15, 7))\n",
    "\n",
    "    for i, df in enumerate([rand, sort]):\n",
    "        ax = axs[i]\n",
    "        mat_data = np.flip(df.loc[0.5]['cov']['median'].unstack().values); \n",
    "        print('sum:'+str(mat_data.sum())); print('trace:'+str(np.diagonal(mat_data).sum()));\n",
    "        \n",
    "        std_data = np.flip(df.loc[0.5]['cov']['std'].unstack().values)\n",
    "        avg_magnitude = np.log10(abs(mat_data).mean()).round(); #c = int(-avg_magnitude)\n",
    "\n",
    "        if scale == 'log': \n",
    "            c = 3\n",
    "        else:\n",
    "            c = -18\n",
    "        \n",
    "        mat_data_sc = mat_data*10**(c)\n",
    "        std_data_sc = std_data*10**(c)\n",
    "\n",
    "#         display(mat_data_sc)\n",
    "        im = ax.imshow(mat_data_sc, vmin = -1, vmax = 1, cmap = 'RdYlGn')\n",
    "\n",
    "        # Show all ticks...\n",
    "        ax.set_xticks(range(Q))\n",
    "        ax.set_yticks(range(Q))\n",
    "        # ... and label them with the respective list entries\n",
    "        ax.set_xticklabels(range(1, Q + 1))\n",
    "        ax.set_yticklabels(range(1, Q + 1))\n",
    "\n",
    "        # Anotate mean values\n",
    "        for i in range(Q):\n",
    "            for j in range(Q):\n",
    "                if abs(mat_data_sc[i, j]) > .8:\n",
    "                    text = ax.text(j, i-.1, round(mat_data_sc[i, j], 1),\n",
    "                                   ha=\"center\", va=\"center\", color=\"w\")\n",
    "                else: \n",
    "                    text = ax.text(j, i-.1, round(mat_data_sc[i, j], 1),\n",
    "                                   ha=\"center\", va=\"center\", color=\"k\")\n",
    "\n",
    "        # Anotate stds\n",
    "        for i in range(Q):\n",
    "            for j in range(Q):\n",
    "                if abs(mat_data_sc[i, j]) > .8:\n",
    "                    text = ax.text(j, i+.2, r'$\\pm\\ $'+str(round(std_data_sc[i, j], 1)),\n",
    "                                   ha=\"center\", va=\"center\", color=\"w\")\n",
    "                else: \n",
    "                    text = ax.text(j, i+.2, r'$\\pm\\ $'+str(round(std_data_sc[i, j], 1)),\n",
    "                                   ha=\"center\", va=\"center\", color=\"k\")\n",
    "\n",
    "        ax.set_title(\"cross covariances\" + r'$\\times 10^{{{}}}$'.format(c))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig('./../../../WRITING/paper1_writing/figures/parts_crosscov_'+scale+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bs_result = pd.concat(out_list)\n",
    "# data = bs_result.groupby(['q', 's']).agg(['mean', 'std']).drop('m', axis = 1)\n",
    "data = bs_result.groupby(['q', 's']).median().drop('m', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 5, figsize = (30, 5))\n",
    "\n",
    "\n",
    "for i, s in enumerate([.02, .05, .1, .25, .5]):\n",
    "    ax = axs[i]\n",
    "    data_s = data.loc[data.index.get_level_values('s') == s]\n",
    "    lognqs = np.log10(data_s.nqs).values\n",
    "\n",
    "    ax.plot(lognqs, np.log10(np.sqrt(data_s['yqs_var'].values)), marker = 'o', label = 'total') # actual std of quantiles\n",
    "    ax.plot(lognqs, np.log10(np.sqrt(data_s['noise_var'].values)), marker = 'o', label = 'n') # std of statistical noise\n",
    "    ax.plot(lognqs, np.log10(np.sqrt(data_s['base_var'].values)), marker = 'o', label = 'bias') # std of bias\n",
    "    ax.plot(lognqs, np.log10(np.sqrt(data_s['common_R_var'].values)), label = 'common') # std of the common trend\n",
    "    ax.plot(lognqs, np.log10(np.sqrt(data_s['common_R_var'].values) /5), label = 'common', lw = .5, c = '.5') # std of the common trend\n",
    "    ax.plot(lognqs, np.log10(np.sqrt(data_s['common_R_var'].values + data_s['base_var'].values + data_s['noise_var'].values)), linestyle = '--', c = '.5', label = 'diag exp') # std of the common trend\n",
    "    ax.plot(lognqs, .5*i + 8.3-lognqs/2, label = '1/sqrt(n)', lw = .5, c = '.5')\n",
    "    ax.legend(loc = (1, .5))\n",
    "    ax.set_ylim(7, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Same thing but I want no biases. So, I want an effective nq and use a lognormal > 3 CDF levels, over T = 17 as base on which to add shocks. Also, now the shocks will be randomised (within quantile?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero BIAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../../data/processed/ID_Y.csv')\n",
    "# df = pd.read_csv('./../../data/processed/.csv')\n",
    "\n",
    "sales = df.loc[df.IMPORT == 0].groupby(['ID', 'YEAR'])['VART'].sum().unstack()\n",
    "sales = sales.loc[sales.sum(1).sort_values().index]\n",
    "\n",
    "logsales = np.log10(sales)\n",
    "demlogsales = logsales.subtract(logsales.mean(1), axis = 0)\n",
    "\n",
    "sizes = sales.loc[sales.sum(1).sort_values().index].sum(1)\n",
    "\n",
    "Q = 10\n",
    "parts = pd.cut(sizes.cumsum()/sizes.sum(), Q, labels = range(Q)).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Effective Nq\n",
    "eff_nq = sales.groupby(parts).count().mean(1).round().astype(int)\n",
    "eff_nq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size dist Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "# from scipy.stats import norm\n",
    "# from scipy.stats import pareto\n",
    "from scipy.special import erf\n",
    "\n",
    "#  - Lognormal clipped x > 3. \n",
    "sigma = 1.2810683494198207 # 1.3149476902828778\n",
    "mu = 4.536908110675739 # 4.470439741406725\n",
    "# 11.5% of guys that would be below the .3 threshold.\n",
    "z = (mu - 3)/sigma\n",
    "cum_th = 1 - .5*(1 + erf(z/np.sqrt(2)))\n",
    "\n",
    "# emp_nq_med = nq.groupby(level = 0).median()\n",
    "N = int(eff_nq.sum().round())\n",
    "\n",
    "N_ = int(round(N/(1 - cum_th))) # We use a larger N ..\n",
    "# From the theoretical N and the ppf we can know the theoretical quantiles\n",
    "x_logn_clip3 = np.array([norm.ppf(q, mu, sigma) for q in np.arange(0, 1, 1/N_) + .5/N_])\n",
    "x_logn_clip3 = x_logn_clip3[(-N - 1):-1]\n",
    "\n",
    "\n",
    "T = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = pd.Series(np.power(10, x_logn_clip3))\n",
    "\n",
    "s0_n = s0.groupby(pd.cut(s0.cumsum()/s0.sum(), Q, labels = range(Q))).count()\n",
    "bs_n = eff_nq.round(1)\n",
    "\n",
    "show = pd.concat([bs_n, s0_n], axis = 1); show.columns = ['Bootstrap nq', 'Lognormal >3 synthetic dist']\n",
    "show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simu_shocks = (s/micro_s)*logsales.sub(logsales.mean(1), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition into Common shocks and noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dem = logsales.sub(logsales.mean(1), axis = 0)\n",
    "sample_shocks = dem.loc[dem.count(1) > 1].unstack().dropna() # (1.6 m real shocks)\n",
    "micro_s = logsales.sub(logsales.mean(1), axis=0).unstack().std()\n",
    "zero_shock = pd.DataFrame(pd.concat(T * [pd.Series(x_logn_clip3)], axis = 1))\n",
    "zero_shock.columns = sales.columns\n",
    "\n",
    "out_list = []\n",
    "\n",
    "n = 50\n",
    "Q = 10\n",
    "\n",
    "for m in range(n):\n",
    "    print(m)\n",
    "    for s in [.02, .05, .1, .25, .5]:\n",
    "\n",
    "#         df_bs = sales.sample(frac = .5)\n",
    "#         df_bs = df_bs.loc[df_bs.sum(1).sort_values().index] #sorting\n",
    "\n",
    "#         logsales = np.log10(sales)\n",
    "        simu_shocks = pd.DataFrame((s/micro_s)*np.random.choice(sample_shocks.values, zero_shock.shape))\n",
    "        simu_shocks.columns = sales.columns\n",
    "\n",
    "#         micro_s = logsales.sub(logsales.mean(1), axis=0).unstack().std()\n",
    "#         zero_shock = logsales.notna().multiply(logsales.mean(1), axis = 0).replace(0, np.nan)\n",
    "        \n",
    "#         simu_shocks = (s/micro_s)*logsales.sub(logsales.mean(1), axis = 0)\n",
    "\n",
    "        total = np.power(10, zero_shock + simu_shocks)\n",
    "        base = np.power(10, zero_shock)\n",
    "        noise = np.power(10, zero_shock + simu_shocks) - np.power(10, zero_shock)\n",
    "        \n",
    "        total['q'] = pd.cut(total.sum(1).cumsum(), Q, labels = range(Q))\n",
    "\n",
    "        common_R = noise.groupby(total['q']).sum().median()\n",
    "        # optional, broadcast to 2d Q x T\n",
    "        common_R = pd.DataFrame(np.outer(np.ones(Q), common_R.values), columns=common_R.index)\n",
    "\n",
    "        noise_qs = noise.groupby(total['q']).sum() - common_R\n",
    "        base_qs = base.groupby(total['q']).sum()\n",
    "        yqs = common_R + noise_qs + base_qs\n",
    "        \n",
    "        nqs = total['q'].value_counts().values\n",
    "#         lognqs = np.log10(nqs)\n",
    "        \n",
    "        out = pd.concat([yqs.var(1), noise_qs.var(1), base_qs.var(1), common_R.var(1)], axis = 1)\n",
    "        out.columns = ['yqs_var', 'noise_var', 'base_var', 'common_R_var']\n",
    "        out['q'] = range(Q); out['m'] = m; out['nqs'] = nqs; out['s'] = s;\n",
    "\n",
    "        out_list += [out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_result = pd.concat(out_list)\n",
    "# data = bs_result.groupby(['q', 's']).agg(['mean', 'std']).drop('m', axis = 1)\n",
    "data = bs_result.groupby(['q', 's']).median().drop('m', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 5, figsize = (30, 5))\n",
    "\n",
    "\n",
    "for i, s in enumerate([.02, .05, .1, .25, .5]):\n",
    "    ax = axs[i]\n",
    "    data_s = data.loc[data.index.get_level_values('s') == s]\n",
    "    lognqs = np.log10(data_s.nqs).values\n",
    "\n",
    "    ax.plot(lognqs, np.log10(np.sqrt(data_s['yqs_var'].values)), marker = 'o', label = 'total') # actual std of quantiles\n",
    "    ax.plot(lognqs, np.log10(np.sqrt(data_s['noise_var'].values)), marker = 'o', label = 'n') # std of statistical noise\n",
    "#     ax.plot(lognqs, np.log10(np.sqrt(data_s['base_var'].values)), marker = 'o', label = 'bias') # std of bias\n",
    "    ax.plot(lognqs, np.log10(np.sqrt(data_s['common_R_var'].values)), label = 'common') # std of the common trend\n",
    "    ax.plot(lognqs, np.log10(np.sqrt(data_s['common_R_var'].values) /5), label = 'common', lw = .5, c = '.5') # std of the common trend\n",
    "    ax.plot(lognqs, np.log10(np.sqrt(data_s['common_R_var'].values + data_s['base_var'].values + data_s['noise_var'].values)), linestyle = '--', c = '.5', label = 'diag exp') # std of the common trend\n",
    "    ax.plot(lognqs, .5*i + 8.3-lognqs/2, label = '1/sqrt(n)', lw = .5, c = '.5')\n",
    "    ax.legend(loc = (1, .5))\n",
    "    ax.set_ylim(7, 11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition into only noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dem = logsales.sub(logsales.mean(1), axis = 0)\n",
    "sample_shocks = dem.loc[dem.count(1) > 1].unstack().dropna() # (1.6 m real shocks)\n",
    "micro_s = logsales.sub(logsales.mean(1), axis=0).unstack().std()\n",
    "zero_shock = pd.DataFrame(pd.concat(T * [pd.Series(x_logn_clip3)], axis = 1))\n",
    "zero_shock.columns = sales.columns\n",
    "\n",
    "out_list = []\n",
    "\n",
    "n = 50\n",
    "Q = 10\n",
    "\n",
    "for m in range(n):\n",
    "    print(m)\n",
    "    for s in [.02, .05, .1, .25, .5]:\n",
    "\n",
    "#         df_bs = sales.sample(frac = .5)\n",
    "#         df_bs = df_bs.loc[df_bs.sum(1).sort_values().index] #sorting\n",
    "\n",
    "#         logsales = np.log10(sales)\n",
    "        simu_shocks = pd.DataFrame((s/micro_s)*np.random.choice(sample_shocks.values, zero_shock.shape))\n",
    "        simu_shocks.columns = sales.columns\n",
    "\n",
    "        total = np.power(10, zero_shock + simu_shocks)\n",
    "        base = np.power(10, zero_shock) # Is really frozen in this case\n",
    "        noise = np.power(10, zero_shock + simu_shocks) - np.power(10, zero_shock)\n",
    "        \n",
    "        total['q'] = pd.cut(total.sum(1).cumsum(), Q, labels = range(Q))\n",
    "\n",
    "        noise_qs = noise.groupby(total['q']).sum()\n",
    "        base_qs = base.groupby(total['q']).sum()\n",
    "        yqs = noise_qs + base_qs\n",
    "        \n",
    "        nqs = total['q'].value_counts().values\n",
    "#         lognqs = np.log10(nqs)\n",
    "        \n",
    "        out = pd.concat([yqs.var(1), noise_qs.var(1), base_qs.var(1)], axis = 1)\n",
    "        out.columns = ['yqs_var', 'noise_var', 'base_var']\n",
    "        out['q'] = range(Q); out['m'] = m; out['nqs'] = nqs; out['s'] = s;\n",
    "\n",
    "        out_list += [out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_result = pd.concat(out_list).reset_index(drop = True)\n",
    "# data = bs_result.groupby(['q', 's']).agg(['mean', 'std']).drop('m', axis = 1)\n",
    "data = bs_result.groupby(['q', 's']).median().drop('m', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the 1/n\n",
    "\n",
    "fit_data = np.log10(data[['nqs', 'yqs_var']])\n",
    "\n",
    "for s in fit_data.index.get_level_values('s').unique():\n",
    "    fit_data_s = fit_data.loc[fit_data.index.get_level_values('s') == s]\n",
    "    fit_data_s = fit_data_s.loc[fit_data_s.nqs < 4]\n",
    "\n",
    "    x = fit_data_s.nqs\n",
    "    y = fit_data_s.yqs_var\n",
    "    z = np.polyfit(x, y, 1); p = np.poly1d(z);\n",
    "    \n",
    "    print(s, p/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 5, figsize = (30, 5))\n",
    "\n",
    "\n",
    "for i, s in enumerate([.02, .05, .1, .25, .5]):\n",
    "    ax = axs[i]\n",
    "    data_s = data.loc[data.index.get_level_values('s') == s]\n",
    "    lognqs = np.log10(data_s.nqs).values\n",
    "\n",
    "    ax.plot(lognqs, np.log10(np.sqrt(data_s['yqs_var'].values)), marker = 'o', label = 'total') # actual std of quantiles\n",
    "    ax.plot(lognqs, np.log10(np.sqrt(data_s['noise_var'].values)), marker = 'o', label = 'n') # std of statistical noise\n",
    "\n",
    "    ax.plot(lognqs, .5*i + 8.3-lognqs/2, label = '1/sqrt(n)', lw = .5, c = '.5')\n",
    "    ax.legend(loc = (1, .5))\n",
    "    ax.set_ylim(7, 11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total =  quantile bases: [ np.power(10, zero_shock) ]  +  quantile noises: [ np.power(10, zero_shock + simu_shocks) - np.power(10, zero_shock) ] = line + shocks + quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total = np.power(10, zero_shock + simu_shocks)\n",
    "# base = np.power(10, zero_shock)\n",
    "# noise = np.power(10, zero_shock + simu_shocks) - np.power(10, zero_shock)\n",
    "\n",
    "# total_q = total.groupby(df_bs['q']).sum()\n",
    "# base_q = base.groupby(df_bs['q']).sum()\n",
    "# noise_q = noise.groupby(df_bs['q']).sum()\n",
    "\n",
    "# q = 5\n",
    "# print(total_q.iloc[q].var())\n",
    "# print(pd.concat([noise_q.iloc[q], base_q.iloc[q]], axis = 1).cov().sum().sum())\n",
    "\n",
    "# for q in range(Q):\n",
    "#     print(q)\n",
    "#     (( np.power(10, zero_shock + simu_shocks)).groupby(total['q']).sum()).iloc[q].plot()#.T.plot(legend = False)\n",
    "\n",
    "#     (np.power(10, zero_shock).groupby(total['q']).sum()).iloc[q].plot()#.T.plot(legend = False)\n",
    "\n",
    "#     ((np.power(10, zero_shock + simu_shocks) - np.power(10, zero_shock)).groupby(total['q']).sum()).iloc[q].plot(linestyle = '--')#.T.plot(legend = False)\n",
    "#     plt.ylim(0, 2.5e10)\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Non scaled, sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_list = []\n",
    "qs_bs_list = []\n",
    "parts_bs_list = []\n",
    "# nq_list = []\n",
    "# trend_std_list = []\n",
    "\n",
    "n = 50\n",
    "Q = 10\n",
    "\n",
    "for m in range(n):\n",
    "\n",
    "    df_bs = sales.sample(frac = .5)\n",
    "    df_bs = df_bs.loc[df_bs.sum(1).sort_values().index] #sorting\n",
    "\n",
    "    df_bs['q'] = pd.cut(df_bs.sum(1).cumsum(), Q, labels = range(Q))\n",
    "\n",
    "    nq = df_bs.groupby('q').count().mean(1) ## Effective nq\n",
    "#     nq = df_bs.q.value_counts()  ## Total count n\n",
    "\n",
    "    df_bs = df_bs.groupby('q').sum()\n",
    "\n",
    "    common_trend = df_bs.median()\n",
    "\n",
    "    x = np.log10(common_trend).index.astype(int).values\n",
    "    y = np.log10(common_trend).values\n",
    "    z = np.polyfit(x, y, 1); p = np.poly1d(z);\n",
    "\n",
    "    exp_curve = pd.Series(np.power(10, p(x)), index = x)\n",
    "\n",
    "#         df_bs = df_bs/(.5 * exp_curve)\n",
    "    shocks = common_trend/exp_curve\n",
    "    df_bs = df_bs/(shocks*exp_curve) # == df_bs/common_trend\n",
    "\n",
    "    parts_bs_2 = pd.concat([exp_curve, shocks, df_bs.sum()], axis = 1).T\n",
    "    parts_bs_2['m'] = m; parts_bs_2['s'] = s\n",
    "    parts_bs_list += [parts_bs_2]\n",
    "\n",
    "\n",
    "    df_bs['nq'] = nq; df_bs['m'] = m; df_bs['s'] = s;\n",
    "    qs_bs_list += [df_bs]\n",
    "\n",
    "#         trend_std_list += [np.log10(common_trend/np.power(10, p(x))).std()]\n",
    "\n",
    "\n",
    "parts_bs_2 = pd.concat(parts_bs_list)\n",
    "qs_bs_2 = pd.concat(qs_bs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Microshocks\n",
    "\n",
    "demlogsales['parts'] = parts\n",
    "std_data = demlogsales.loc[demlogsales.iloc[:, :-1].count(1) > 1]\n",
    "std_data.reset_index().set_index(['ID', 'parts']).stack().groupby(level = 'parts').std()\n",
    "\n",
    "\n",
    "# Un array de donde samplear shocks empiricos.\n",
    "emp_shocks = std_data.reset_index().set_index(['ID', 'parts']).stack().values\n",
    "len(emp_shocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BS for sorted quantile partitions\n",
    "# n = 50\n",
    "# out = []\n",
    "\n",
    "# for m in range(n):\n",
    "#     df_bs = sales.sample(frac = .5)\n",
    "#     df_bs = df_bs.loc[df_bs.sum(1).sort_values().index] #sorting\n",
    "#     df_bs['q'] = pd.cut(df_bs.sum(1).cumsum(), Q, labels = range(Q))\n",
    "\n",
    "# #     nq = df_bs.groupby('q').count().mean(1) ## Effective nq\n",
    "# #     nq = df_bs.q.value_counts()  ## Total count n\n",
    "\n",
    "#     df_bs = df_bs.groupby('q').sum()\n",
    "#     out += [df_bs]\n",
    "    \n",
    "# bs_data = pd.concat(out)\n",
    "\n",
    "# demedianed_log10 = np.log10(bs_data/pd.concat(n*[bs_data.groupby(level = 0).median()]))\n",
    "# # var_nq = pd.concat([demedianed_log10.var(1), nq], axis = 1); var_nq.columns = ['var', 'nq']\n",
    "# # quantile shocks\n",
    "\n",
    "# qshocks = demedianed_log10.diff(axis = 1).stack().reset_index()\n",
    "# # qshocks['bin'] = pd.cut(qshocks[0], np.arange(-.1, .1, .001))\n",
    "# qshocks['bin'] = pd.cut(qshocks[0], np.arange(-.1, .1, .001))\n",
    "\n",
    "# qshocks_hist = pd.DataFrame(qshocks.groupby(['bin', 'q']).size().unstack())\n",
    "\n",
    "# qshocks_hist.rolling(6, center = True).mean().plot(figsize = (7, 5), marker = '.')\n",
    "# plt.yscale('log')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BS for sorted quantile partitions\n",
    "n = 50\n",
    "out = []\n",
    "\n",
    "for m in range(n):\n",
    "    df_bs = sales.sample(frac = .5)\n",
    "    df_bs = df_bs.loc[df_bs.sum(1).sort_values().index] #sorting\n",
    "    df_bs['q'] = pd.cut(df_bs.sum(1).cumsum(), Q, labels = range(Q))\n",
    "\n",
    "#     nq = df_bs.groupby('q').count().mean(1) ## Effective nq\n",
    "#     nq = df_bs.q.value_counts()  ## Total count n\n",
    "\n",
    "    df_bs = df_bs.groupby('q').sum()\n",
    "    out += [df_bs]\n",
    "    \n",
    "bs_data = pd.concat(out)\n",
    "\n",
    "demedianed_log10 = np.log10(bs_data/pd.concat(n*[bs_data.groupby(level = 0).median()]))\n",
    "qshocks = demedianed_log10.diff(axis = 1).stack().reset_index()\n",
    "qshocks['bin'] = pd.cut(qshocks[0], np.arange(-.1, .1, .001))\n",
    "\n",
    "\n",
    "# BS for sorted quantile partitions\n",
    "n = 50\n",
    "out = []\n",
    "\n",
    "for m in range(n):\n",
    "    df_bs = sales.sample(frac = .5)\n",
    "#     df_bs = df_bs.loc[df_bs.sum(1).sort_values().index] #sorting\n",
    "    df_bs['q'] = pd.cut(df_bs.sum(1).cumsum(), Q, labels = range(Q))\n",
    "    df_bs = df_bs.groupby('q').sum()\n",
    "    out += [df_bs]\n",
    "    \n",
    "bs_data_unsort_q = pd.concat(out)\n",
    "\n",
    "demedianed_log10 = np.log10(bs_data_unsort_q/pd.concat(n*[bs_data_unsort_q.groupby(level = 0).median()]))\n",
    "qshocks2 = demedianed_log10.diff(axis = 1).stack().reset_index()\n",
    "qshocks2['bin'] = pd.cut(qshocks2[0], np.arange(-.1, .1, .001))\n",
    "\n",
    "\n",
    "\n",
    "## BS for Aggregate\n",
    "\n",
    "n = 20\n",
    "\n",
    "lo_agg = []\n",
    "hi_agg = []\n",
    "for m in range(n):\n",
    "    df_bs = sales.sample(frac = .5)\n",
    "    df_bs.sum()\n",
    "    \n",
    "    agg = np.log10(df_bs.sum()/df_bs.sum().mean())\n",
    "    \n",
    "    lo_agg += [agg.quantile(.1)]\n",
    "    hi_agg += [agg.quantile(.9)]\n",
    "    \n",
    "lo_agg = np.array(lo_agg).mean()\n",
    "hi_agg = np.array(hi_agg).mean()\n",
    "\n",
    "## Firm sales xlims come from other notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "fig, axs = plt.subplots(1, 4, figsize = (28, 5))\n",
    "\n",
    "\n",
    "## Aggregate\n",
    "ax = axs[0]\n",
    "x = np.arange(lo_agg, hi_agg, 0.001)\n",
    "y = np.power(10, x)\n",
    "ax.plot(x, y, label = 'agg', lw = 2, alpha = .5, color = 'k')\n",
    "ax.plot(x, np.log(10) * (x) + 1, c = 'r', linestyle = '--', lw = .25)\n",
    "ax.plot(x, (np.log(10) * (x))**2/2 + np.log(10) * (x) + 1, c = 'r')\n",
    "ax.set_xlim(-.12, .1)\n",
    "ax.set_ylim(.7, 1.25)\n",
    "ax.legend()\n",
    "\n",
    "### Sorted quantile partition\n",
    "\n",
    "lo_q = qshocks.groupby('q')[0].quantile(.1)\n",
    "hi_q = qshocks.groupby('q')[0].quantile(.9)\n",
    "\n",
    "ax = axs[1]\n",
    "for q in range(Q):\n",
    "    x = np.arange(lo_q[q], hi_q[q], 0.001)\n",
    "    y = np.power(10, x)\n",
    "    ax.plot(x, y, label = q, lw = 10 - q, alpha = .5, color = 'k')\n",
    "    ax.plot(x, np.log(10) * (x) + 1, c = 'r', linestyle = '--', lw = .25)\n",
    "    ax.plot(x, (np.log(10) * x)**2/2 + np.log(10) * x + 1, c = 'r')\n",
    "ax.set_xlim(-.12, .1)\n",
    "ax.set_ylim(.7, 1.25)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "### Unsorted quantile partition\n",
    "\n",
    "lo_q = qshocks2.groupby('q')[0].quantile(.1)\n",
    "hi_q = qshocks2.groupby('q')[0].quantile(.9)\n",
    "\n",
    "ax = axs[2]\n",
    "for q in range(Q):\n",
    "    x = np.arange(lo_q[q], hi_q[q], 0.001)\n",
    "    y = np.power(10, x)\n",
    "    ax.plot(x, y, label = q, lw = 10 - q, alpha = .5, color = 'k')\n",
    "    ax.plot(x, np.log(10) * (x) + 1, c = 'r', linestyle = '--', lw = .25)\n",
    "    ax.plot(x, (np.log(10) * x)**2/2 + np.log(10) * x + 1, c = 'r')\n",
    "ax.set_xlim(-.12, .1)\n",
    "ax.set_ylim(.7, 1.25)\n",
    "ax.legend()\n",
    "\n",
    "### Firm level\n",
    "ax = axs[3]\n",
    "# half_nominal_measure_l = df_demlog_nominal_shrink.loc[df_demlog_nominal_shrink['gap'].cumsum()/df_demlog_nominal_shrink['gap'].sum() < .25].demlog.max()\n",
    "# half_nominal_measure_r = df_demlog_nominal_growth.loc[df_demlog_nominal_growth['gap'].cumsum()/df_demlog_nominal_growth['gap'].sum() < .75].demlog.max()\n",
    "\n",
    "x90 = np.arange(-1.7, 0.7, .1)\n",
    "x75 = np.arange(-.73, 0.32, .05)  # Only 25% of the value in added in shocks smaller than this, 25% of the value in added in shocks larger than this\n",
    "\n",
    "ax.plot(x90, np.power(10, x90), lw = 2, alpha = .5, color = 'k')\n",
    "ax.plot(x75, np.power(10, x75), lw = 5, alpha = .5, color = 'k')\n",
    "\n",
    "ax.plot(x90, 1 + np.log(10)*x90 + (np.log(10)*x90)**2/2 + (np.log(10)*x90)**3/6, linestyle = '--', alpha = .5, c = 'r', lw =1, label = 'deg 3 poly')\n",
    "ax.plot(x90, 1 + np.log(10)*x90 + (np.log(10)*x90)**2/2, linestyle = ':', alpha = .5, c = 'r', lw =1, label = 'deg 2 poly')\n",
    "ax.plot(x90, 1 + np.log(10)*x90, linestyle = ':', alpha = .5, c = 'r', label = 'line')\n",
    "\n",
    "# Create a Rectangle patch\n",
    "rect = patches.Rectangle((-.12, .7), .1- (-.12), 1.25 - .7,linewidth=1,edgecolor='k',facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 4.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data = qshocks_hist.rolling(6, center = True).mean()\n",
    "# data = data.loc[[-.05 < m.mid < .05 for m in data.index]]\n",
    "\n",
    "# quad_params = []\n",
    "\n",
    "# for q in range(Q):\n",
    "#     x = [m.mid for m in data[q].dropna().index]\n",
    "#     y = data[q].dropna().values\n",
    "#     z = np.polyfit(x, y, 2)\n",
    "#     p = np.poly1d(z)\n",
    "# #     print(p)\n",
    "#     quad_params += [z]\n",
    "    \n",
    "# quad_params = np.array(quad_params)\n",
    "\n",
    "\n",
    "# # NQs from bootstrap\n",
    "# var_nq['m'] = [i//Q for i in range(n*Q)]\n",
    "# nq = var_nq.reset_index().set_index(['q', 'm'])['nq'].unstack().median(1)\n",
    "\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2, figsize = (16, 5))\n",
    "\n",
    "# ax = axs[0]\n",
    "# ax.plot(range(Q), -quad_params[:, 1]/(2*quad_params[:, 0]), label = 'mu_q')\n",
    "# ax.plot(range(Q), 1/np.sqrt(-2*quad_params[:, 0]), marker = '.', label = 'sigma_q')\n",
    "# ax.set_xlabel('q'); ax.set_ylabel('log pts')\n",
    "# ax.axhline(0, c = '.5', linestyle = '--')\n",
    "# ax.legend()\n",
    "\n",
    "\n",
    "# ax = axs[1]\n",
    "# ax.plot(np.log10(nq.values), -quad_params[:, 1]/(2*quad_params[:, 0]), label = 'mu_q')\n",
    "# ax.plot(np.log10(nq.values), 1/np.sqrt(-2*quad_params[:, 0]), marker = '.', label = 'sigma_q')\n",
    "# ax.set_xlabel('log_n'); ax.set_ylabel('log pts')\n",
    "# ax.axhline(0, c = '.5', linestyle = '--')\n",
    "# ax.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit to theoretical deduction with \"narrow bin\" approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Microshocks\n",
    "\n",
    "demlogsales['parts'] = parts\n",
    "std_data = demlogsales.loc[demlogsales.iloc[:, :-1].count(1) > 1]\n",
    "std_data.reset_index().set_index(['ID', 'parts']).stack().groupby(level = 'parts').std()\n",
    "\n",
    "\n",
    "# Un array de donde samplear shocks empiricos.\n",
    "emp_shocks = std_data.reset_index().set_index(['ID', 'parts']).stack().values\n",
    "len(emp_shocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile_lo (x): \n",
    "    return np.percentile(x, q=10)\n",
    "def percentile_hi (x): \n",
    "    return np.percentile(x, q=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emp_nqs = np.round(nq.sort_values()).astype(int)\n",
    "partition = eff_nq.astype(int)\n",
    "ss = np.arange(0.1, .8, .1)\n",
    "M = 25\n",
    "\n",
    "T = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Experiments (gaussian and laplace deviations from mean)\n",
    "\n",
    "results = []\n",
    "\n",
    "s0 = emp_shocks.std()\n",
    "\n",
    "for dist in ['norm', 'lapl', 'emp']:\n",
    "    for q in range(Q)[1:]:\n",
    "        part = partition.index[q]\n",
    "        n = partition.values[q]\n",
    "#         x0 = np.log10(sizes.loc[parts == part]).values # comment for narrow bin limit\n",
    "        print(q, n)\n",
    "        for s in ss:\n",
    "            for m in range(M):\n",
    "                if dist == 'norm':\n",
    "                    shocks = np.random.normal(0, s, (n, T))\n",
    "                elif dist == 'lapl':\n",
    "                    shocks = np.random.laplace(0, s, (n, T))/np.sqrt(2)\n",
    "                elif dist == 'emp':\n",
    "                    shocks = (np.random.choice(emp_shocks, n * T)*(s/s0)).reshape(n, T)\n",
    "                    \n",
    "                values = np.log10(np.power(10, shocks).sum(0)/n) #M?\n",
    "#                 values = np.log10(np.power(10, x0[:, None] + shocks).sum(0)/np.power(10, x0[:, None]).sum(0)) #M? # comment for narrow bin limit\n",
    "\n",
    "#             t += [np.power(10, x0 + x1).sum()/(n*10**mu0)]\n",
    "                results += [[dist, s, n, m, values.mean(), values.std()]]\n",
    "\n",
    "result = pd.DataFrame(results, columns = ['dist', 's', 'nq', 'repeat', 'mean', 'std'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vs micro sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize = (15, 8))\n",
    "\n",
    "#################\n",
    "# Normal\n",
    "plot_data = result.loc[result.dist == 'norm'].groupby(['nq','s'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])#.reset_index()\n",
    "\n",
    "ax = axs[0][0]\n",
    "ax.set_title('mean, normal shocks')\n",
    "\n",
    "y_data = plot_data['mean']['mean'].unstack()\n",
    "yhi_data = plot_data['mean']['percentile_hi'].unstack() - plot_data['mean']['mean'].unstack()\n",
    "ylo_data = plot_data['mean']['mean'].unstack() - plot_data['mean']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_ylim(-.1, 1.5)\n",
    "ax.axhline(0, color = '.5', lw = .5, linestyle = '--')\n",
    "ax.set_xlabel('micro sigma')\n",
    "    \n",
    "## Analytical expectation\n",
    "ax.plot(x, x**2*np.log(10)/2, color = 'r', linestyle = '--', zorder = 10)\n",
    "\n",
    "###\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(.001, 5)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "\n",
    "\n",
    "#################\n",
    "ax = axs[1][0]\n",
    "ax.set_title('std, normal shocks')\n",
    "\n",
    "# logplot_data = np.log10(plot_data)\n",
    "y_data = plot_data['std']['mean'].unstack()\n",
    "yhi_data = plot_data['std']['percentile_hi'].unstack() - plot_data['std']['mean'].unstack()\n",
    "ylo_data = plot_data['std']['mean'].unstack() - plot_data['std']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_xlabel('micro sigma')\n",
    "\n",
    "## Analytical expectation\n",
    "# for n0 in partition.values[1:]:\n",
    "#     ax.plot(x, np.sqrt(np.exp(x**2)*(np.exp(x**2) - 1)/n0), color = 'r', linestyle = '--', zorder = 10, lw = .5)\n",
    "for n0 in partition.values[1:]:\n",
    "    ax.plot(x, np.sqrt((np.exp(x**2) - 1)/n0), color = 'r', linestyle = '--', zorder = 10, lw = .5)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "    \n",
    "    \n",
    "#################\n",
    "# Empirical Shocks\n",
    "plot_data = result.loc[result.dist == 'emp'].groupby(['nq', 's'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])#.reset_index()\n",
    "\n",
    "ax = axs[0][1]\n",
    "ax.set_title('mean, Empirical shocks')\n",
    "\n",
    "y_data = plot_data['mean']['mean'].unstack()\n",
    "yhi_data = plot_data['mean']['percentile_hi'].unstack() - plot_data['mean']['mean'].unstack()\n",
    "ylo_data = plot_data['mean']['mean'].unstack() - plot_data['mean']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_ylim(-.1, 1.5)\n",
    "ax.axhline(0, color = '.5', lw = .5, linestyle = '--')\n",
    "ax.axvspan(.5, .6, facecolor='.85', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('micro sigma')\n",
    "\n",
    "###\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(.001, 1.5)\n",
    "    \n",
    "ax.set_xscale('log')\n",
    "\n",
    "#################\n",
    "    \n",
    "ax = axs[1][1]\n",
    "ax.set_title('std, Empirical shocks')\n",
    "\n",
    "# logplot_data = np.log10(plot_data)\n",
    "y_data = plot_data['std']['mean'].unstack()\n",
    "yhi_data = plot_data['std']['percentile_hi'].unstack() - plot_data['std']['mean'].unstack()\n",
    "ylo_data = plot_data['std']['mean'].unstack() - plot_data['std']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "\n",
    "ax.axvspan(.5, .6, facecolor='.85', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('micro sigma')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "\n",
    "#################\n",
    "# Laplace\n",
    "plot_data = result.loc[result.dist == 'lapl'].groupby(['nq','s'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])#.reset_index()\n",
    "\n",
    "\n",
    "ax = axs[0][2]\n",
    "ax.set_title('mean, Laplace shocks')\n",
    "\n",
    "y_data = plot_data['mean']['mean'].unstack()\n",
    "yhi_data = plot_data['mean']['percentile_hi'].unstack() - plot_data['mean']['mean'].unstack()\n",
    "ylo_data = plot_data['mean']['mean'].unstack() - plot_data['mean']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_ylim(-.1, 1.5)\n",
    "ax.axhline(0, color = '.5', lw = .5, linestyle = '--')\n",
    "ax.set_xlabel('micro sigma')\n",
    "\n",
    "\n",
    "## Analytical expectation\n",
    "ax.plot(x, np.log10(1/(1 - (x*np.log(10))**2/2)), color = 'r', linestyle = '--', zorder = 10)\n",
    "\n",
    "###\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(.001, 5)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "\n",
    "\n",
    "#################\n",
    "    \n",
    "ax = axs[1][2]\n",
    "ax.set_title('std, Laplace shocks')\n",
    "\n",
    "# logplot_data = np.log10(plot_data)\n",
    "y_data = plot_data['std']['mean'].unstack()\n",
    "yhi_data = plot_data['std']['percentile_hi'].unstack() - plot_data['std']['mean'].unstack()\n",
    "ylo_data = plot_data['std']['mean'].unstack() - plot_data['std']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "    \n",
    "ax.set_xlabel('micro sigma')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./../../../WRITING/paper1_writing/figures/power_sums_vs_sigma.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vs log(nq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize = (15, 8))\n",
    "\n",
    "#################\n",
    "# Normal\n",
    "plot_data = result.loc[result.dist == 'norm'].groupby(['s','nq'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])#.reset_index()\n",
    "\n",
    "ax = axs[0][0]\n",
    "ax.set_title('mean, normal shocks')\n",
    "\n",
    "y_data = plot_data['mean']['mean'].unstack()\n",
    "yhi_data = plot_data['mean']['percentile_hi'].unstack() - plot_data['mean']['mean'].unstack()\n",
    "ylo_data = plot_data['mean']['mean'].unstack() - plot_data['mean']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_ylim(-.1, 1.5)\n",
    "ax.axhline(0, color = '.5', lw = .5, linestyle = '--')\n",
    "ax.set_xlabel('log(nq)')\n",
    "    \n",
    "ax.set_xscale('log')\n",
    "#################\n",
    "ax = axs[1][0]\n",
    "ax.set_title('std, normal shocks')\n",
    "\n",
    "# logplot_data = np.log10(plot_data)\n",
    "y_data = plot_data['std']['mean'].unstack()\n",
    "yhi_data = plot_data['std']['percentile_hi'].unstack() - plot_data['std']['mean'].unstack()\n",
    "ylo_data = plot_data['std']['mean'].unstack() - plot_data['std']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_xlabel('log(nq)')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "for s0 in [0.1, 0.6]:\n",
    "    ax.plot(x, np.sqrt((np.exp(s0**2) - 1)/x), color = 'r', linestyle = '--', zorder = 10, lw = .5)\n",
    "\n",
    "#################\n",
    "# Empirical Shocks\n",
    "plot_data = result.loc[result.dist == 'emp'].groupby(['s','nq'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])#.reset_index()\n",
    "\n",
    "\n",
    "ax = axs[0][1]\n",
    "ax.set_title('mean, Empirical shocks')\n",
    "\n",
    "y_data = plot_data['mean']['mean'].unstack()\n",
    "yhi_data = plot_data['mean']['percentile_hi'].unstack() - plot_data['mean']['mean'].unstack()\n",
    "ylo_data = plot_data['mean']['mean'].unstack() - plot_data['mean']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_ylim(-.1, 1.5)\n",
    "ax.axhline(0, color = '.5', lw = .5, linestyle = '--')\n",
    "ax.set_xlabel('log(nq)')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "#################\n",
    "    \n",
    "ax = axs[1][1]\n",
    "ax.set_title('std, Empirical shocks')\n",
    "\n",
    "# logplot_data = np.log10(plot_data)\n",
    "y_data = plot_data['std']['mean'].unstack()\n",
    "yhi_data = plot_data['std']['percentile_hi'].unstack() - plot_data['std']['mean'].unstack()\n",
    "ylo_data = plot_data['std']['mean'].unstack() - plot_data['std']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_xlabel('log(nq)')\n",
    "\n",
    "# sqrt(sobs2 - (strend)2) = (sq)\n",
    "# ax.plot(np.log10(emp_nq_med), np.log10(np.sqrt(emp_std_mn**2 - np.median(trend_std)**2))) \n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "for s0 in [0.1, 0.6]:\n",
    "    ax.plot(x, np.sqrt((np.exp(s0**2) - 1)/x), color = 'r', linestyle = '--', zorder = 10, lw = .5)\n",
    "\n",
    "#################\n",
    "# Laplace\n",
    "plot_data = result.loc[result.dist == 'lapl'].groupby(['s','nq'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])#.reset_index()\n",
    "\n",
    "\n",
    "ax = axs[0][2]\n",
    "ax.set_title('mean, Laplace shocks')\n",
    "\n",
    "y_data = plot_data['mean']['mean'].unstack()\n",
    "yhi_data = plot_data['mean']['percentile_hi'].unstack() - plot_data['mean']['mean'].unstack()\n",
    "ylo_data = plot_data['mean']['mean'].unstack() - plot_data['mean']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "    \n",
    "ax.set_ylim(-.1, 1.5)\n",
    "ax.axhline(0, color = '.5', lw = .5, linestyle = '--')\n",
    "ax.set_xlabel('log(nq)')\n",
    "ax.set_xscale('log')\n",
    "\n",
    "#################\n",
    "    \n",
    "ax = axs[1][2]\n",
    "ax.set_title('std, Laplace shocks')\n",
    "\n",
    "# logplot_data = np.log10(plot_data)\n",
    "y_data = plot_data['std']['mean'].unstack()\n",
    "yhi_data = plot_data['std']['percentile_hi'].unstack() - plot_data['std']['mean'].unstack()\n",
    "ylo_data = plot_data['std']['mean'].unstack() - plot_data['std']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_xlabel('log(nq)')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "for s0 in [0.1, 0.6]:\n",
    "    ax.plot(x, np.sqrt((np.exp(s0**2) - 1)/x), color = 'r', linestyle = '--', zorder = 10, lw = .5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./../../../WRITING/paper1_writing/figures/power_sums_vs_nq.png')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat, non narrow bins, i.e. sizes in x0 used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap on empirical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs_data = qs_bs.loc[qs_bs.s == .1].drop(['nq', 'm', 's'], axis = 1)\n",
    "\n",
    "# # Empirical stds\n",
    "# emp_std_lo = np.log10(bs_data).std(1).groupby(level = 0).quantile(.1)\n",
    "# emp_std_mn = np.log10(bs_data).std(1).groupby(level = 0).mean()\n",
    "# emp_std_hi = np.log10(bs_data).std(1).groupby(level = 0).quantile(.9)\n",
    "\n",
    "# # Nqs\n",
    "# emp_nq_med = nq.groupby(level = 0).median()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_bs_2['std'] = np.log10(qs_bs_2[np.arange(1997, 2014)]).std(1)\n",
    "grouped_2 = qs_bs_2.reset_index().groupby(['q', 's'])\n",
    "\n",
    "emp_plot_data_2 = grouped_2['std'].median().unstack()\n",
    "emp_plot_data_lo_2 = grouped_2['std'].quantile(.25).unstack()\n",
    "emp_plot_data_hi_2 = grouped_2['std'].quantile(.75).unstack()\n",
    "nqs = grouped_2['nq'].median().unstack().iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.log10(emp_plot_data_hi_2.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_bs['std'] = np.log10(qs_bs[np.arange(1997, 2014)]).std(1)\n",
    "grouped = qs_bs.reset_index().groupby(['q', 's'])\n",
    "emp_plot_data = grouped['std'].median().unstack()\n",
    "emp_plot_data_lo = grouped['std'].quantile(.25).unstack()\n",
    "emp_plot_data_hi = grouped['std'].quantile(.75).unstack()\n",
    "nqs = grouped['nq'].median().unstack().iloc[:, 0]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize = (18, 6))\n",
    "\n",
    "#################\n",
    "    \n",
    "ax = axs[0]\n",
    "ax.set_title('std, Empirical shocks')\n",
    "\n",
    "ax.set_xlabel('log(nq)')\n",
    "\n",
    "for s in emp_plot_data.columns:\n",
    "    ax.plot(np.log10(nqs), np.log10(emp_plot_data[s]), marker = '*', label = 'mu_q', color = '.5', lw = 0)\n",
    "    ax.fill_between(np.log10(nqs), np.log10(emp_plot_data_hi[s]), np.log10(emp_plot_data_lo[s]), color = '.8', alpha = .25, label = 'common_shocks_std')\n",
    "    \n",
    "    ax.plot(np.log10(nqs), np.log10(np.sqrt((np.exp(s**2) - 1)/nqs)), color = 'r', linestyle = '--', zorder = 10, lw = .5)\n",
    "ax.fill_between(np.log10(nqs), np.log10(emp_plot_data_hi_2.values.flatten()), np.log10(emp_plot_data_lo_2.values.flatten()), color = 'g', alpha = .15, label = 'common_shocks_std')\n",
    "\n",
    "#################\n",
    "    \n",
    "ax = axs[1]\n",
    "ax.set_title('std, Empirical shocks')\n",
    "\n",
    "emp_plot_data.index = range(Q)\n",
    "\n",
    "for q in emp_plot_data.index[::3]:\n",
    "    ax.plot(np.log10(emp_plot_data.columns), np.log10(emp_plot_data.loc[q]), marker = '*', label = 'mu_q', color = '.5', lw = 0)\n",
    "    ax.fill_between(np.log10(emp_plot_data.columns), np.log10(emp_plot_data_hi.loc[q]), np.log10(emp_plot_data_lo.loc[q]), color = '.8', alpha = .25, label = 'common_shocks_std')\n",
    "\n",
    "ax.set_xlabel('micro sigma')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic lognormal for x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "#  - Lognormal clipped x > 3. \n",
    "sigma = 1.2810683494198207 # 1.3149476902828778\n",
    "mu = 4.536908110675739 # 4.470439741406725\n",
    "# 11.5% of guys that would be below the .3 threshold.\n",
    "z = (mu - 3)/sigma\n",
    "cum_th = 1 - .5*(1 + erf(z/np.sqrt(2)))\n",
    "\n",
    "\n",
    "N = int(eff_nq.sum().round())\n",
    "\n",
    "N_ = int(round(N/(1 - cum_th))) # We use a larger N ..\n",
    "# From the theoretical N and the ppf we can know the theoretical quantiles\n",
    "x_logn_clip3 = np.array([norm.ppf(q, mu, sigma) for q in np.arange(0, 1, 1/N_) + .5/N_])\n",
    "x_logn_clip3 = x_logn_clip3[(-N - 1):-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = pd.Series(np.power(10, x_logn_clip3))\n",
    "\n",
    "s0_n = s0.groupby(pd.cut(s0.cumsum()/s0.sum(), Q, labels = range(Q))).count()\n",
    "bs_n = eff_nq.round(1)\n",
    "\n",
    "show = pd.concat([bs_n, s0_n], axis = 1); show.columns = ['Bootstrap nq', 'Lognormal >3 synthetic dist']\n",
    "show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_nqs = np.round(eff_nq.sort_values()).astype(int)\n",
    "ss = np.arange(0.1, .8, .1)\n",
    "M = 5\n",
    "\n",
    "T = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiments (gaussian and laplace deviations from mean)\n",
    "partition_df = s0_n # index are quantile ixs and value has their n.\n",
    "\n",
    "results = []\n",
    "\n",
    "sigma0 = emp_shocks.std()\n",
    "\n",
    "for dist in ['emp']:\n",
    "    for q in range(Q):\n",
    "        part = partition_df.index[q]\n",
    "        n = partition_df.round().astype(int).values[q]\n",
    "        x0 = pd.Series(x_logn_clip3).loc[pd.cut(s0.cumsum()/s0.sum(), Q, labels = range(Q)) == part].values\n",
    "#         print(q, n)\n",
    "        for s in ss:\n",
    "            for m in range(M):\n",
    "                if dist == 'norm':\n",
    "                    shocks = np.random.normal(0, s, (n, T))\n",
    "                elif dist == 'lapl':\n",
    "                    shocks = np.random.laplace(0, s, (n, T))/np.sqrt(2)\n",
    "                elif dist == 'emp':\n",
    "                    shocks = (np.random.choice(emp_shocks, n * T)*(s/sigma0)).reshape(n, T)\n",
    "                    \n",
    "#                 values = np.log10(np.power(10, shocks).sum(0)/n) #M?\n",
    "                values = np.log10(np.power(10, x0[:, None] + shocks).sum(0)/np.power(10, x0[:, None]).sum(0)) #M?\n",
    "                results += [[dist, s, q, n, m, values.mean(), values.std()]]\n",
    "\n",
    "result = pd.DataFrame(results, columns = ['dist', 's', 'q', 'nq', 'repeat', 'mean', 'std'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize = (18, 12))\n",
    "\n",
    "#################\n",
    "# Empirical Shocks\n",
    "# plot_data = result.loc[(result.dist == 'emp') & (result.s >= .5)].groupby(['s','nq'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])#.reset_index()\n",
    "plot_data = result.loc[(result.dist == 'emp')].groupby(['s', 'nq'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])\n",
    "\n",
    "ax = axs[0][0]\n",
    "ax.set_title('mean, Empirical shocks')\n",
    "\n",
    "y_data = plot_data['mean']['mean'].unstack()\n",
    "yhi_data = plot_data['mean']['percentile_hi'].unstack() - plot_data['mean']['mean'].unstack()\n",
    "ylo_data = plot_data['mean']['mean'].unstack() - plot_data['mean']['percentile_lo'].unstack()\n",
    "x = np.log10(y_data.columns.values)\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_ylim(-.1, 1.5)\n",
    "ax.axhline(0, color = '.5', lw = .5, linestyle = '--')\n",
    "ax.set_xlabel('log(nq)')\n",
    "    \n",
    "# # Empirical\n",
    "# ax.fill_between(np.log10(eff_nq), emp_mean_hi, emp_mean_lo, color = '#1f77b4', alpha = .25, label = 'empirical')\n",
    "# ax.plot(np.log10(eff_nq), emp_mean_mn, marker = '*', label = 'mu_q', color = 'g', lw = 0)\n",
    "\n",
    "#################\n",
    "    \n",
    "ax = axs[1][0]\n",
    "ax.set_title('std, Empirical shocks')\n",
    "\n",
    "logplot_data = np.log10(plot_data)\n",
    "y_data = logplot_data['std']['mean'].unstack()\n",
    "yhi_data = logplot_data['std']['percentile_hi'].unstack() - logplot_data['std']['mean'].unstack()\n",
    "ylo_data = logplot_data['std']['mean'].unstack() - logplot_data['std']['percentile_lo'].unstack()\n",
    "x = np.log10(y_data.columns.values)\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_xlabel('log(nq)')\n",
    "\n",
    "# # Empirical\n",
    "# ax.fill_between(np.log10(eff_nq), np.log10(emp_std_hi), np.log10(emp_std_lo), color = '#1f77b4', alpha = .25, label = 'empirical')\n",
    "# ax.plot(np.log10(eff_nq), np.log10(emp_std_mn), marker = '*', label = 'mu_q', color = 'g', lw = 0)\n",
    "\n",
    "# ax.fill_between(np.log10(eff_nq), np.log10(np.quantile(trend_std, .9)), np.log10(np.quantile(trend_std, .1)), color = '.8', alpha = .25, label = 'common_shocks_std')\n",
    "# ax.axhline(np.mean(trend_std), color = '.5', alpha = .25, lw = .5, label = 'common_shocks_std')\n",
    "\n",
    "# sqrt(sobs2 - (strend)2) = (sq)\n",
    "# ax.plot(np.log10(eff_nq), np.log10(np.sqrt(emp_std_mn**2 - np.median(trend_std)**2))) \n",
    "\n",
    "\n",
    "#################\n",
    "# Empirical Shocks\n",
    "plot_data = result.loc[result.dist == 'emp'].groupby(['nq', 's'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])#.reset_index()\n",
    "\n",
    "\n",
    "ax = axs[0][1]\n",
    "ax.set_title('mean, Empirical shocks')\n",
    "\n",
    "y_data = plot_data['mean']['mean'].unstack()\n",
    "yhi_data = plot_data['mean']['percentile_hi'].unstack() - plot_data['mean']['mean'].unstack()\n",
    "ylo_data = plot_data['mean']['mean'].unstack() - plot_data['mean']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_ylim(-.1, 1.5)\n",
    "ax.axhline(0, color = '.5', lw = .5, linestyle = '--')\n",
    "ax.set_xlabel('micro sigma')\n",
    "\n",
    "###\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(.001, 1.5)\n",
    "    \n",
    "ax.set_xscale('log')\n",
    "\n",
    "#################\n",
    "    \n",
    "ax = axs[1][1]\n",
    "ax.set_title('std, Empirical shocks')\n",
    "\n",
    "logplot_data = np.log10(plot_data)\n",
    "y_data = logplot_data['std']['mean'].unstack()\n",
    "yhi_data = logplot_data['std']['percentile_hi'].unstack() - logplot_data['std']['mean'].unstack()\n",
    "ylo_data = logplot_data['std']['mean'].unstack() - logplot_data['std']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_xlabel('micro sigma')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize = (27, 12))\n",
    "\n",
    "#################\n",
    "# Normal\n",
    "plot_data = result.loc[result.dist == 'norm'].groupby(['nq','s'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])#.reset_index()\n",
    "\n",
    "ax = axs[0][0]\n",
    "ax.set_title('mean, normal shocks')\n",
    "\n",
    "y_data = plot_data['mean']['mean'].unstack()\n",
    "yhi_data = plot_data['mean']['percentile_hi'].unstack() - plot_data['mean']['mean'].unstack()\n",
    "ylo_data = plot_data['mean']['mean'].unstack() - plot_data['mean']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_ylim(-.1, 1.5)\n",
    "ax.axhline(0, color = '.5', lw = .5, linestyle = '--')\n",
    "ax.set_xlabel('micro sigma')\n",
    "    \n",
    "## Analytical expectation\n",
    "ax.plot(x, x**2*np.log(10)/2, color = 'r', linestyle = '--', zorder = 10)\n",
    "\n",
    "###\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(.001, 5)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "\n",
    "\n",
    "#################\n",
    "ax = axs[1][0]\n",
    "ax.set_title('std, normal shocks')\n",
    "\n",
    "logplot_data = np.log10(plot_data)\n",
    "y_data = logplot_data['std']['mean'].unstack()\n",
    "yhi_data = logplot_data['std']['percentile_hi'].unstack() - logplot_data['std']['mean'].unstack()\n",
    "ylo_data = logplot_data['std']['mean'].unstack() - logplot_data['std']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_xlabel('micro sigma')\n",
    "\n",
    "## Analytical expectation\n",
    "for n0 in emp_nqs:\n",
    "    ax.plot(x, np.log10(np.sqrt(np.exp(x**2)*(np.exp(x**2) - 1)/n0)), color = 'r', linestyle = '--', zorder = 10, lw = .5)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "\n",
    "    \n",
    "#################\n",
    "# Laplace\n",
    "plot_data = result.loc[result.dist == 'lapl'].groupby(['nq','s'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])#.reset_index()\n",
    "\n",
    "\n",
    "ax = axs[0][1]\n",
    "ax.set_title('mean, Laplace shocks')\n",
    "\n",
    "y_data = plot_data['mean']['mean'].unstack()\n",
    "yhi_data = plot_data['mean']['percentile_hi'].unstack() - plot_data['mean']['mean'].unstack()\n",
    "ylo_data = plot_data['mean']['mean'].unstack() - plot_data['mean']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_ylim(-.1, 1.5)\n",
    "ax.axhline(0, color = '.5', lw = .5, linestyle = '--')\n",
    "ax.set_xlabel('micro sigma')\n",
    "\n",
    "\n",
    "## Analytical expectation\n",
    "ax.plot(x, np.log10(1/(1 - (x*np.log(10))**2/2)), color = 'r', linestyle = '--', zorder = 10)\n",
    "\n",
    "###\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(.001, 5)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "\n",
    "\n",
    "#################\n",
    "    \n",
    "ax = axs[1][1]\n",
    "ax.set_title('std, Laplace shocks')\n",
    "\n",
    "logplot_data = np.log10(plot_data)\n",
    "y_data = logplot_data['std']['mean'].unstack()\n",
    "yhi_data = logplot_data['std']['percentile_hi'].unstack() - logplot_data['std']['mean'].unstack()\n",
    "ylo_data = logplot_data['std']['mean'].unstack() - logplot_data['std']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "    \n",
    "ax.set_xlabel('micro sigma')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "\n",
    "\n",
    "#################\n",
    "# Empirical Shocks\n",
    "plot_data = result.loc[result.dist == 'emp'].groupby(['nq', 's'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])#.reset_index()\n",
    "\n",
    "\n",
    "ax = axs[0][2]\n",
    "ax.set_title('mean, Empirical shocks')\n",
    "\n",
    "y_data = plot_data['mean']['mean'].unstack()\n",
    "yhi_data = plot_data['mean']['percentile_hi'].unstack() - plot_data['mean']['mean'].unstack()\n",
    "ylo_data = plot_data['mean']['mean'].unstack() - plot_data['mean']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_ylim(-.1, 1.5)\n",
    "ax.axhline(0, color = '.5', lw = .5, linestyle = '--')\n",
    "ax.set_xlabel('micro sigma')\n",
    "\n",
    "###\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(.001, 1.5)\n",
    "    \n",
    "ax.set_xscale('log')\n",
    "\n",
    "#################\n",
    "    \n",
    "ax = axs[1][2]\n",
    "ax.set_title('std, Empirical shocks')\n",
    "\n",
    "logplot_data = np.log10(plot_data)\n",
    "y_data = logplot_data['std']['mean'].unstack()\n",
    "yhi_data = logplot_data['std']['percentile_hi'].unstack() - logplot_data['std']['mean'].unstack()\n",
    "ylo_data = logplot_data['std']['mean'].unstack() - logplot_data['std']['percentile_lo'].unstack()\n",
    "x = y_data.columns.values\n",
    "\n",
    "for s in y_data.index.values:\n",
    "    ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_xlabel('micro sigma')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Proximos testeos: size distribution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para ellos, es todo igual, nomas que hay una distribucion de sizes, los shocks se agregan a ellas y se divide por (X/Q) en vez de (s0*n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Size distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.stats import pareto\n",
    "from scipy.special import erf\n",
    "\n",
    "# The size dists are:\n",
    "#  - Lognormal clipped x > 3. \n",
    "sigma = 1.2810683494198207 # 1.3149476902828778\n",
    "mu = 4.536908110675739 # 4.470439741406725\n",
    "# 11.5% of guys that would be below the .3 threshold.\n",
    "z = (mu - 3)/sigma\n",
    "cum_th = 1 - .5*(1 + erf(z/np.sqrt(2)))\n",
    "\n",
    "# We'll try different N's, until we can match the avg level of France imports \n",
    "\n",
    "N = int(1e5)\n",
    "N_ = int(round(N/(1 - cum_th))) # We use a larger N ..\n",
    "draw = np.random.normal(mu, sigma, N_) # so that approximately we'll have N guys above x = 3\n",
    "draw = np.sort(draw)[-N:]\n",
    "\n",
    "# Now we can have the n(Q, q)\n",
    "def get_n(x1, Q):\n",
    "    x_lin = np.power(10, x1)\n",
    "    bins = pd.cut(pd.Series(np.cumsum(x_lin)), Q)\n",
    "    ns = bins.value_counts().values\n",
    "    return bins, ns\n",
    "\n",
    "# From the theoretical N and the ppf we can know the theoretical quantiles\n",
    "x_logn_clip3 = np.array([norm.ppf(q, mu, sigma) for q in np.arange(0, 1, 1/N_) + .5/N_])\n",
    "x_logn_clip3 = x_logn_clip3[(-N - 1):-1]\n",
    "\n",
    "## N tail for pareto and lognormal tail\n",
    "N_tail = get_n(x_logn_clip3, 10)[1][1:].sum()\n",
    "\n",
    "n = N_tail\n",
    "x_logn_clip3_90 = x_logn_clip3[-N_tail:]\n",
    "\n",
    "# PARETO\n",
    "z_0 = -1.1042021 #-1.1771\n",
    "# value_qs_1 = x_logn_clip3_90.min() # 6.761 in the original fit\n",
    "value_qs_1 = 6.67465\n",
    "\n",
    "norm = 1.375\n",
    "x1 = np.array([pareto.ppf(b = -z_0, scale = 10**value_qs_1, q = q) for q in np.arange(0, 1, 1/(norm*N_tail)) + .5/(norm*N_tail)])\n",
    "np.log10(pareto.rvs(b = -z_0, size = n, scale = 10**value_qs_1) + 1)\n",
    "x1 = x1[int(-norm*N_tail - 1):-1]\n",
    "\n",
    "x_pareto = np.log10(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emp_nqs = np.round(nq.sort_values()).astype(int)\n",
    "ss = np.arange(0.1, .8, .2)\n",
    "M = 50\n",
    "\n",
    "T = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiments (gaussian and laplace deviations from mean)\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "for dist in ['norm', 'lapl']:\n",
    "    for j, size_dist in enumerate([x_logn_clip3, x_logn_clip3_90, x_pareto]):\n",
    "        sdist_name = ['Logn', 'Logn90', 'Pareto'][j]\n",
    "        bins, partition_ns = get_n(size_dist, Q)\n",
    "        for q, part in enumerate(bins.unique()):\n",
    "            x0 = pd.Series(size_dist).loc[bins == part].values\n",
    "            n = partition_ns[q]\n",
    "            for s in ss:\n",
    "                for m in range(M):\n",
    "                    if dist == 'norm':\n",
    "                        shocks = np.random.normal(0, s, (n, T))\n",
    "                    elif dist == 'lapl':\n",
    "                        shocks = np.random.laplace(0, s, (n, T))/np.sqrt(2)\n",
    "    #                 values = np.log10(np.power(10, shocks).sum(0)/n) #M?\n",
    "                    # same as before but now x0 is a vector of sizes in the quantile, determined by the size dist, N and the Q chosen.\n",
    "                    values = np.log10(np.power(10, x0[:, None] + shocks).sum(0)/np.power(10, x0[:, None]).sum(0)) #M?\n",
    "    #             t += [np.power(10, x0 + x1).sum()/(n*10**mu0)]\n",
    "\n",
    "\n",
    "                    results += [[dist, sdist_name, s, n, m, values.mean(), values.std()]]\n",
    "\n",
    "sdist_result = pd.DataFrame(results, columns = ['dist', 'size_dist', 's', 'nq', 'repeat', 'mean', 'std'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize = (18, 12))\n",
    "\n",
    "#################\n",
    "# Normal\n",
    "ax = axs[0][0]\n",
    "ax.set_title('mean, normal shocks')\n",
    "\n",
    "for sdist in ['Logn', 'Logn90', 'Pareto']:\n",
    "    plot_data = sdist_result.loc[(sdist_result.size_dist == sdist) & (sdist_result.dist == 'norm')].groupby(['s','nq'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])#.reset_index()\n",
    "    \n",
    "    y_data = plot_data['mean']['mean'].unstack()\n",
    "    yhi_data = plot_data['mean']['percentile_hi'].unstack() - plot_data['mean']['mean'].unstack()\n",
    "    ylo_data = plot_data['mean']['mean'].unstack() - plot_data['mean']['percentile_lo'].unstack()\n",
    "    x = np.log10(y_data.columns.values)\n",
    "\n",
    "    for s in y_data.index.values:\n",
    "        ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "    \n",
    "ax.set_ylim(-.1, 1.5)\n",
    "ax.axhline(0, color = '.5', lw = .5, linestyle = '--')\n",
    "ax.set_xlabel('log(nq)')\n",
    "    \n",
    "# #################\n",
    "ax = axs[1][0]\n",
    "ax.set_title('std, normal shocks')\n",
    "\n",
    "for sdist in ['Logn', 'Logn90', 'Pareto']:\n",
    "    plot_data = sdist_result.loc[(sdist_result.size_dist == sdist) & (sdist_result.dist == 'norm')].groupby(['s','nq'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])#.reset_index()\n",
    "    logplot_data = np.log10(plot_data)\n",
    "    y_data = logplot_data['std']['mean'].unstack()\n",
    "    yhi_data = logplot_data['std']['percentile_hi'].unstack() - logplot_data['std']['mean'].unstack()\n",
    "    ylo_data = logplot_data['std']['mean'].unstack() - logplot_data['std']['percentile_lo'].unstack()\n",
    "    x = np.log10(y_data.columns.values)\n",
    "\n",
    "    for s in y_data.index.values:\n",
    "        ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_xlabel('log(nq)')\n",
    "\n",
    "# # Empirical\n",
    "# ax.fill_between(np.log10(emp_nq_med), np.log10(emp_std_hi), np.log10(emp_std_lo), color = '#1f77b4', alpha = .25, label = 'empirical')\n",
    "# ax.plot(np.log10(emp_nq_med), np.log10(emp_std_mn), marker = '*', label = 'mu_q', color = 'g', lw = 0)\n",
    "\n",
    "# #################\n",
    "# # Laplace\n",
    "\n",
    "ax = axs[0][1]\n",
    "ax.set_title('mean, Laplace shocks')\n",
    "\n",
    "for sdist in ['Logn', 'Logn90', 'Pareto']:\n",
    "    plot_data = sdist_result.loc[(sdist_result.size_dist == sdist) & (sdist_result.dist == 'lapl')].groupby(['s','nq'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])#.reset_index()\n",
    "    \n",
    "    y_data = plot_data['mean']['mean'].unstack()\n",
    "    yhi_data = plot_data['mean']['percentile_hi'].unstack() - plot_data['mean']['mean'].unstack()\n",
    "    ylo_data = plot_data['mean']['mean'].unstack() - plot_data['mean']['percentile_lo'].unstack()\n",
    "    x = np.log10(y_data.columns.values)\n",
    "\n",
    "    for s in y_data.index.values:\n",
    "        ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "\n",
    "ax.set_ylim(-.1, 1.5)\n",
    "ax.axhline(0, color = '.5', lw = .5, linestyle = '--')\n",
    "ax.set_xlabel('log(nq)')\n",
    "    \n",
    "# # Empirical\n",
    "# ax.fill_between(np.log10(emp_nq_med), emp_mean_hi, emp_mean_lo, color = '#1f77b4', alpha = .25, label = 'empirical')\n",
    "# ax.plot(np.log10(emp_nq_med), emp_mean_mn, marker = '*', label = 'mu_q', color = 'g', lw = 0)\n",
    "\n",
    "# #################\n",
    "    \n",
    "ax = axs[1][1]\n",
    "ax.set_title('std, Laplace shocks')\n",
    "\n",
    "for sdist in ['Logn', 'Logn90', 'Pareto']:\n",
    "    plot_data = sdist_result.loc[(sdist_result.size_dist == sdist) & (sdist_result.dist == 'lapl')].groupby(['s','nq'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])#.reset_index()\n",
    "    logplot_data = np.log10(plot_data)\n",
    "    y_data = logplot_data['std']['mean'].unstack()\n",
    "    yhi_data = logplot_data['std']['percentile_hi'].unstack() - logplot_data['std']['mean'].unstack()\n",
    "    ylo_data = logplot_data['std']['mean'].unstack() - logplot_data['std']['percentile_lo'].unstack()\n",
    "    x = np.log10(y_data.columns.values)\n",
    "\n",
    "    for s in y_data.index.values:\n",
    "        ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_xlabel('log(nq)')\n",
    "\n",
    "# # Empirical\n",
    "# ax.fill_between(np.log10(emp_nq_med), np.log10(emp_std_hi), np.log10(emp_std_lo), color = '#1f77b4', alpha = .25, label = 'empirical')\n",
    "# ax.plot(np.log10(emp_nq_med), np.log10(emp_std_mn), marker = '*', label = 'mu_q', color = 'g', lw = 0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize = (18, 12))\n",
    "\n",
    "#################\n",
    "# Normal\n",
    "ax = axs[0][0]\n",
    "ax.set_title('mean, normal shocks')\n",
    "\n",
    "for sdist in ['Logn', 'Logn90', 'Pareto']:\n",
    "    plot_data = sdist_result.loc[(sdist_result.size_dist == sdist) & (sdist_result.dist == 'norm')].groupby(['nq', 's'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])#.reset_index()\n",
    "    \n",
    "    y_data = plot_data['mean']['mean'].unstack()\n",
    "    yhi_data = plot_data['mean']['percentile_hi'].unstack() - plot_data['mean']['mean'].unstack()\n",
    "    ylo_data = plot_data['mean']['mean'].unstack() - plot_data['mean']['percentile_lo'].unstack()\n",
    "    x = np.log10(y_data.columns.values)\n",
    "\n",
    "    for s in y_data.index.values:\n",
    "        ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "    \n",
    "ax.set_ylim(-.1, 1.5)\n",
    "ax.axhline(0, color = '.5', lw = .5, linestyle = '--')\n",
    "ax.set_xlabel('micro sigma')\n",
    "    \n",
    "# #################\n",
    "ax = axs[1][0]\n",
    "ax.set_title('std, normal shocks')\n",
    "\n",
    "for sdist in ['Logn', 'Logn90', 'Pareto']:\n",
    "    plot_data = sdist_result.loc[(sdist_result.size_dist == sdist) & (sdist_result.dist == 'norm')].groupby(['nq', 's'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])#.reset_index()\n",
    "    logplot_data = np.log10(plot_data)\n",
    "    y_data = logplot_data['std']['mean'].unstack()\n",
    "    yhi_data = logplot_data['std']['percentile_hi'].unstack() - logplot_data['std']['mean'].unstack()\n",
    "    ylo_data = logplot_data['std']['mean'].unstack() - logplot_data['std']['percentile_lo'].unstack()\n",
    "    x = np.log10(y_data.columns.values)\n",
    "\n",
    "    for s in y_data.index.values:\n",
    "        ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_xlabel('micro sigma')\n",
    "\n",
    "# # Empirical\n",
    "# ax.fill_between(np.log10(emp_nq_med), np.log10(emp_std_hi), np.log10(emp_std_lo), color = '#1f77b4', alpha = .25, label = 'empirical')\n",
    "# ax.plot(np.log10(emp_nq_med), np.log10(emp_std_mn), marker = '*', label = 'mu_q', color = 'g', lw = 0)\n",
    "\n",
    "# #################\n",
    "# # Laplace\n",
    "\n",
    "ax = axs[0][1]\n",
    "ax.set_title('mean, Laplace shocks')\n",
    "\n",
    "for sdist in ['Logn', 'Logn90', 'Pareto']:\n",
    "    plot_data = sdist_result.loc[(sdist_result.size_dist == sdist) & (sdist_result.dist == 'lapl')].groupby(['nq', 's'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])#.reset_index()\n",
    "    \n",
    "    y_data = plot_data['mean']['mean'].unstack()\n",
    "    yhi_data = plot_data['mean']['percentile_hi'].unstack() - plot_data['mean']['mean'].unstack()\n",
    "    ylo_data = plot_data['mean']['mean'].unstack() - plot_data['mean']['percentile_lo'].unstack()\n",
    "    x = np.log10(y_data.columns.values)\n",
    "\n",
    "    for s in y_data.index.values:\n",
    "        ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "\n",
    "ax.set_ylim(-.1, 1.5)\n",
    "ax.axhline(0, color = '.5', lw = .5, linestyle = '--')\n",
    "ax.set_xlabel('micro sigma')\n",
    "    \n",
    "# # Empirical\n",
    "# ax.fill_between(np.log10(emp_nq_med), emp_mean_hi, emp_mean_lo, color = '#1f77b4', alpha = .25, label = 'empirical')\n",
    "# ax.plot(np.log10(emp_nq_med), emp_mean_mn, marker = '*', label = 'mu_q', color = 'g', lw = 0)\n",
    "\n",
    "# #################\n",
    "    \n",
    "ax = axs[1][1]\n",
    "ax.set_title('std, Laplace shocks')\n",
    "\n",
    "for sdist in ['Logn', 'Logn90', 'Pareto']:\n",
    "    plot_data = sdist_result.loc[(sdist_result.size_dist == sdist) & (sdist_result.dist == 'lapl')].groupby(['nq', 's'])[['mean', 'std']].agg([np.mean, percentile_lo, percentile_hi])#.reset_index()\n",
    "    logplot_data = np.log10(plot_data)\n",
    "    y_data = logplot_data['std']['mean'].unstack()\n",
    "    yhi_data = logplot_data['std']['percentile_hi'].unstack() - logplot_data['std']['mean'].unstack()\n",
    "    ylo_data = logplot_data['std']['mean'].unstack() - logplot_data['std']['percentile_lo'].unstack()\n",
    "    x = np.log10(y_data.columns.values)\n",
    "\n",
    "    for s in y_data.index.values:\n",
    "        ax.errorbar(x, y_data.loc[s], yerr=[ylo_data.loc[s], yhi_data.loc[s]], lw = .5, color = '.5', marker = 's', markersize = 3)\n",
    "ax.set_xlabel('micro sigma')\n",
    "\n",
    "# # Empirical\n",
    "# ax.fill_between(np.log10(emp_nq_med), np.log10(emp_std_hi), np.log10(emp_std_lo), color = '#1f77b4', alpha = .25, label = 'empirical')\n",
    "# ax.plot(np.log10(emp_nq_med), np.log10(emp_std_mn), marker = '*', label = 'mu_q', color = 'g', lw = 0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
